{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole purpose of this deep_prose is a demo of the what I want to have t generative interactive deep learning \n",
    "\n",
    "Questions about AWS\n",
    "1. How do you use git to store your notebooks?\n",
    "2. It works like regular git?\n",
    "\n",
    "Release items:\n",
    "\n",
    "- DONE Transformers must be at version 3.5.1\n",
    "- IN-Progress creating a situation\n",
    "\n",
    "TODO Create a website / UI \n",
    "\n",
    "IN PROGRESS : Figure out a situation design\n",
    "\n",
    "Create two situations.\n",
    "\n",
    "Future items\n",
    "TODO: Epsilon state transitions (you unlock a new fsm with rules (interaction with the significant other) going from asking out to date and so forth\n",
    "\n",
    "TODO: Differentiable state machines (make the state transtions of the game POMDP truly dynamic? )\n",
    "\n",
    "TODO: Bertdb (store all results and index it by sentiment. Also store every session with an engagement time)\n",
    "\n",
    "\n",
    "TODO: Create minimal deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \"feature-extraction\": will return a FeatureExtractionPipeline.\n",
    "\n",
    "    \"sentiment-analysis\": will return a TextClassificationPipeline.\n",
    "\n",
    "    \"ner\": will return a TokenClassificationPipeline.\n",
    "\n",
    "    \"question-answering\": will return a QuestionAnsweringPipeline.\n",
    "\n",
    "    \"fill-mask\": will return a FillMaskPipeline.\n",
    "\n",
    "    \"summarization\": will return a SummarizationPipeline.\n",
    "\n",
    "    \"translation_xx_to_yy\": will return a TranslationPipeline.\n",
    "\n",
    "    \"text-generation\": will return a TextGenerationPipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "v0 would be a proof of concept using the below personality.\n",
    "\n",
    "Work combining blenderbot with a q/a seralizing to a databse responses.\n",
    "\n",
    "Has impostor syndrome and is anxious because when she has to fix something in reailty she also agonises over the choices she makes. She wants to take a break . 5' 7'' Her power is field manipulation but every time she uses it it causes more pain.\n",
    "\n",
    "To get her to go out with you. You must figure out her flaws\n",
    "TO get to the next date you have to solve one of them.\n",
    "\n",
    "? how do we make the above show emergent gameplay?\n",
    "combine language model with \n",
    "Write a bunch of skeletons \n",
    "\n",
    "Masked Language Modeling\n",
    "\n",
    "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for downstream tasks, requiring bi-directional context such as SQuAD (question answering, see Lewis, Lui, Goyal et al., part 4.2).\n",
    "\n",
    "\n",
    "inspiration for Saati\n",
    "\n",
    "Saati: Concept for personality\n",
    "    Ditzy but powerful\n",
    "    Hatoko Kushikawa + Aoi Sakuraba + \n",
    "Saati Namba \n",
    "    \n",
    "akuraba (桜庭 葵, Sakuraba Aoi)\n",
    "Voiced by: Ayako Kawasumi (Japanese); Michelle Ruff (English)[1]\n",
    "The female protagonist of the series. Aoi is a generally demure girl often seen wearing an indigo kimono, and she addresses Kaoru as \"Kaoru-sama\". Due to her sharp culinary and housekeeping skills, she is seen as an ideal Japanese woman. As a Sakuraba, Aoi was forced into an arranged marriage with Kaoru and because he left the Habanashi, she (having fallen deeply in love with him), chased after him. Unlike other female leads in harem anime, she works to control her jealousy and is generally successful. She is very devoted to Kaoru, however, and will do all in her power to prevent being separated from him. In the beginning, Aoi was willing to warm him when he had a fever, becoming nude in the process (she made sure he wasn't looking). When Aoi, Kaoru, and Miyabi moved to the Sakuraba's summer estate the public appearance Miyabi wanted was that Aoi would be the landlady, herself the manager, and Kaoru a tenant. When Tina Foster moved in, Aoi and Miyabi became a real landlord and manager. She has a \"bad\" habit of clutching things in her sleep. Eventually, Kaoru's half-brother proposes to Aoi; when Kaoru stops the engagement, Aoi decides to abandon her family to be with Kaoru.\n",
    "\n",
    "atoko Kushikawa (櫛川 鳩子, Kushikawa Hatoko)\n",
    "Voiced by: Hisako Kanemoto (drama CD), Saori Hayami (anime)[2] (Japanese); Melissa Molano (English)[3]\n",
    "A polite airheaded girl who often takes Jurai's chuunibyou antics seriously. She is also Jurai's childhood best friend and has feelings for him as well. Her power is \"Over Element\" (五帝（オーバーエレメント）, Ōbā Eremento, lit. \"Five Emperors\"), giving her the ability to manipulate five main elements; earth, water, fire, wind, and light. She can use these elements simultaneously to create a variety of effects (i.e combining earth and fire to create magma).\n",
    "\n",
    "Saati Namba (ナンバー サーティ, Nanbā Sāti) Activated April 6, 1994\n",
    "Saati is the thirtieth A.I. program Hitoshi created, and is the first program that Hitoshi really liked.\n",
    "Through a freak lightning accident, she is transferred to the real world, and becomes Hitoshi's live-in girlfriend. In the real world she takes the family name \"Namba\" (難波), which is properly pronounced nanpa (\"whirlpool\"). In Japanese naming order her name, Namba Saati, is based on the English pronunciation of \"Number Thirty\" (Nanbā Sāti).\n",
    "A kind girl, size data 82-58-85 cm (32¾\"-23\"-34\") with thick eyebrows (A trait Akamatsu admitted to liking on women, citing Sae Isshiki as an example), and a bit naive, which does lead to frequent problems with Hitoshi. The most notable of which is that when she cooks meals she has the tendency to make them look exactly like the picture, regardless of the taste (having no taste buds), using everything from paint to colored markers until she receives a program upgrade that allows her to taste. She loves Hitoshi and is very loyal and supportive of him.\n",
    "While she is sweet and kind, she is similar to the Love Hina character Naru Narusegawa (whom she resembles) and Negima! Magister Negi Magi character Asuna Kagurazaka in that she beats up Hitoshi if he does something perverted.\n",
    "Hitoshi based her appearance on his mother.\n",
    "\n",
    "Belldandy (ベルダンディー, Berudandī) is a goddess who ends up contractually bound to Keiichi Morisato after he accidentally dials the Goddess Relief Office. Ever since, Belldandy dwells with Keiichi at the Tariki Hongan Temple in the city of Nekomi, Chiba Prefecture near Tokyo. She can cancel the contract at any time, but she states Keiichi is a special person in her heart, and that her purpose is to make him happy. This also evidenced by what she says after the Lord of Terror's Arc is concluded in which she tells Keiichi \"I'm here now because I love you.\"\n",
    "Belldandy is kind, patient, and warm, not only to Keiichi, but to everyone, without exception. She can easily sense other people's emotions, and tries her best to be empathetic to all those around her. However, even though Belldandy tries her best to be as kind as possible, it is revealed that at times, as a result of latent jealousy, she can become very insecure and sad, especially when she is confronted with an implication that involves Keiichi in one way or another. This can sometimes result in power leaks that affect everything within the vicinity, commonly referred to as \"jealousy storms\" in the anime.\n",
    "Belldandy is licensed as a goddess first-class, unlimited, and as such is highly skilled. In the manga is revealed that she also has a Valkyrie License, only because she likes to collect such kind of things. Her power is so great, in fact, that she is required to wear a special earring on her left ear which constantly seals the full brunt of her magical strength. Belldandy's angel is Holy Bell (ホーリーベル, Hōrī Beru); her elemental attribute is wind. Like all angels, Holy Bell augments Belldandy's magical powers when called upon, and like all angels, she also reflects her master's current state\n",
    "\n",
    "how to do this: fetch all anime characters and then retrain gpt to write down above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Core concept you go on various dates / friend oriented adventures that are open ended\n",
    "#Main characters for this tech demo is Saati orare  ???? can we think of a better name\n",
    "#\n",
    "!pip install transformers==3.4.0\n",
    "!pip install ipywidgets\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (TFAutoModelWithLMHead, \n",
    "                         AutoTokenizer, \n",
    "                         pipeline, \n",
    "                         BlenderbotSmallTokenizer, \n",
    "                         BlenderbotForConditionalGeneration, \n",
    "                         Conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/xlnet-base-cased-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /xlnet-base-cased-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/xlnet-base-cased-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/xlnet-base-cased-spiece.model HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/xlnet-base-cased-modelcard.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/xlnet-base-cased-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /xlnet-base-cased-pytorch_model.bin HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '\\n\\nThe female protagonist of the series. Aoi is a generally demure girl often seen wearing an indigo kimono, and she addresses Kaoru as \"Kaoru-sama\". Due to her sharp culinary and housekeeping skills, she is seen as an ideal Japanese woman. As a Sakuraba, Aoi was forced into an arranged marriage with Kaoru and because he left the Habanashi, she (having fallen deeply in love with him), chased after him. Unlike other female leads in harem anime, she works to control her jealousy and is generally successful. She is very devoted to Kaoru, however, and will do all in her power to prevent being separated from him. In the beginning, Aoi was willing to warm him when he had a fever, becoming nude in the process (she made sure he wasn\\'t looking). When Aoi, Kaoru, and Miyabi moved to the Sakuraba\\'s summer estate the public appearance Miyabi wanted was that Aoi would be the landlady, herself the manager, and Kaoru a tenant. When Tina Foster moved in, Aoi and Miyabi became a real landlord and manager. She has a \"bad\" habit of clutching things in her sleep. Eventually, Kaoru\\'s half-brother proposes to Aoi; when Kaoru stops the engagement, Aoi decides to abandon her family to be with Kaoru.\\nA polite airheaded girl who often takes Jurai\\'s chuunibyou antics seriously. She is also Jurai\\'s childhood best friend and has feelings for him as well. Her power is \"Over Element\", Ōbā Eremento, lit. \"Five Emperors\"), giving her the ability to manipulate five main elements; earth, water, fire, wind, and light. She can use these elements simultaneously to create a variety of effects (i.e combining earth and fire to create magma).\\nSaati is the thirtieth A.I. program Hitoshi created, and is the first program that Hitoshi really liked.\\nThrough a freak lightning accident, she is transferred to the real world, and becomes Hitoshi\\'s live-in girlfriend. In the real world she takes the family name \"Namba\", which is properly pronounced nanpa (\"whirlpool\"). In Japanese naming order her name, Namba Saati, is based on the English pronunciation of \"Number Thirty\" (Nanbā Sāti).\\nA kind girl,with thick eyebrows, and a bit naive, which does lead to frequent problems with Hitoshi. The most notable of which is that when she cooks meals she has the tendency to make them look exactly like the picture, regardless of the taste (having no taste buds), using everything from paint to colored markers until she receives a program upgrade that allows her to taste. She loves Hitoshi and is very loyal and supportive of him.\\nWhile she is sweet and kind, she is similar to the Love Hina character Naru Narusegawa (whom she resembles) and Negima! Magister Negi Magi character Asuna Kagurazaka in that she beats up Hitoshi if he does something perverted.\\nHitoshi based her appearance on his mother.\\n<eod> </s> <eos>\\n The female protagonist of the series. The female protagonist of the series. Aoi is a \"sama\", a \"sama\", and she is seen as an ideal Japanese woman. She is also known as \"Sama-sama\", lit. \"The Virgin Mary\", lit. \"The Virgin Mary\". The female protagonist of the series. The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\". She is also known as \"Sama-sama\", lit. \"The Virgin Mary\". The female protagonist of the series is Aoi. She is a \"sama\", lit. \"The Virgin Mary\"'}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-20b092e5a158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlnet_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROMPT_TO_REFINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mlogging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'refined prompt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "from transformers import (TFAutoModelWithLMHead, \n",
    "                         AutoTokenizer, \n",
    "                         pipeline, \n",
    "                         BlenderbotSmallTokenizer, \n",
    "                         BlenderbotForConditionalGeneration, \n",
    "                         Conversation)\n",
    "#NOTE only tested on 3.4.0 of transformers\n",
    "\n",
    "#!pip install transitions[diagrams] \n",
    "#!pip install graphviz pygraphviz \n",
    "#!brew install graphviz\n",
    "from transitions.extensions import HierarchicalGraphMachine as Machine\n",
    "import random\n",
    "from datetime import datetime\n",
    "# Set up logging; The basic log level will be DEBUG\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "\n",
    "description_plot = '''\n",
    "The female protagonist of the series. Aoi is a generally demure girl often seen wearing an indigo kimono, and she addresses Kaoru as \"Kaoru-sama\". Due to her sharp culinary and housekeeping skills, she is seen as an ideal Japanese woman. As a Sakuraba, Aoi was forced into an arranged marriage with Kaoru and because he left the Habanashi, she (having fallen deeply in love with him), chased after him. Unlike other female leads in harem anime, she works to control her jealousy and is generally successful. She is very devoted to Kaoru, however, and will do all in her power to prevent being separated from him. In the beginning, Aoi was willing to warm him when he had a fever, becoming nude in the process (she made sure he wasn't looking). When Aoi, Kaoru, and Miyabi moved to the Sakuraba's summer estate the public appearance Miyabi wanted was that Aoi would be the landlady, herself the manager, and Kaoru a tenant. When Tina Foster moved in, Aoi and Miyabi became a real landlord and manager. She has a \"bad\" habit of clutching things in her sleep. Eventually, Kaoru's half-brother proposes to Aoi; when Kaoru stops the engagement, Aoi decides to abandon her family to be with Kaoru. A polite airheaded girl who often takes Jurai's chuunibyou antics seriously. She is also Jurai's childhood best friend and has feelings for him as well. Her power is \"Over Element\" (五帝（オーバーエレメント）, Ōbā Eremento, lit. \"Five Emperors\"), giving her the ability to manipulate five main elements; earth, water, fire, wind, and light. She can use these elements simultaneously to create a variety of effects (i.e combining earth and fire to create magma). Saati is the thirtieth A.I. program Hitoshi created, and is the first program that Hitoshi really liked.\n",
    "Through a freak lightning accident, she is transferred to the real world, and becomes Hitoshi's live-in girlfriend. In the real world she takes the family name \"Namba\" (難波), which is properly pronounced nanpa (\"whirlpool\"). In Japanese naming order her name, Namba Saati, is based on the English pronunciation of \"Number Thirty\" (Nanbā Sāti).\n",
    "A kind girl, size data 82-58-85 cm (32¾\"-23\"-34\") with thick eyebrows (A trait Akamatsu admitted to liking on women, citing Sae Isshiki as an example), and a bit naive, which does lead to frequent problems with Hitoshi. The most notable of which is that when she cooks meals she has the tendency to make them look exactly like the picture, regardless of the taste (having no taste buds), using everything from paint to colored markers until she receives a program upgrade that allows her to taste. She loves Hitoshi and is very loyal and supportive of him.\n",
    "While she is sweet and kind, she is similar to the Love Hina character Naru Narusegawa (whom she resembles) and Negima! Magister Negi Magi character Asuna Kagurazaka in that she beats up Hitoshi if he does something perverted.\n",
    "Hitoshi based her appearance on his mother.\n",
    "Saati is the thirtieth A.I. program Hitoshi created, and is the first program that Hitoshi really liked.\n",
    "Through a freak lightning accident, she is transferred to the real world, and becomes Hitoshi's live-in girlfriend. In the real world she takes the family name \"Namba\" (難波), which is properly pronounced nanpa (\"whirlpool\"). In Japanese naming order her name, Namba Saati, is based on the English pronunciation of \"Number Thirty\" (Nanbā Sāti).\n",
    "A kind girl, size data 82-58-85 cm (32¾\"-23\"-34\") with thick eyebrows (A trait Akamatsu admitted to liking on women, citing Sae Isshiki as an example), and a bit naive, which does lead to frequent problems with Hitoshi. The most notable of which is that when she cooks meals she has the tendency to make them look exactly like the picture, regardless of the taste (having no taste buds), using everything from paint to colored markers until she receives a program upgrade that allows her to taste. She loves Hitoshi and is very loyal and supportive of him.\n",
    "While she is sweet and kind, she is similar to the Love Hina character Naru Narusegawa (whom she resembles) and Negima! Magister Negi Magi character Asuna Kagurazaka in that she beats up Hitoshi if he does something perverted.\n",
    "Hitoshi based her appearance on his mother. Is a goddess who ends up contractually bound to Keiichi Morisato after he accidentally dials the Goddess Relief Office. Ever since, Belldandy dwells with Keiichi at the Tariki Hongan Temple in the city of Nekomi, Chiba Prefecture near Tokyo. She can cancel the contract at any time, but she states Keiichi is a special person in her heart, and that her purpose is to make him happy. This also evidenced by what she says after the Lord of Terror's Arc is concluded in which she tells Keiichi \"I'm here now because I love you.\"\n",
    "Belldandy is kind, patient, and warm, not only to Keiichi, but to everyone, without exception. She can easily sense other people's emotions, and tries her best to be empathetic to all those around her. However, even though Belldandy tries her best to be as kind as possible, it is revealed that at times, as a result of latent jealousy, she can become very insecure and sad, especially when she is confronted with an implication that involves Keiichi in one way or another. This can sometimes result in power leaks that affect everything within the vicinity, commonly referred to as \"jealousy storms\" in the anime.\n",
    "Belldandy is licensed as a goddess first-class, unlimited, and as such is highly skilled. In the manga is revealed that she also has a Valkyrie License, only because she likes to collect such kind of things. Her power is so great, in fact, that she is required to wear a special earring on her left ear which constantly seals the full brunt of her magical strength. Belldandy's angel is Holy Bell (ホーリーベル, Hōrī Beru); her elemental attribute is wind. Like all angels, Holy Bell augments Belldandy's magical powers when called upon, and like all angels, she also reflects her master's current state\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "the_universe_is_a_glitch = '''\n",
    "The Universe Is a Glitch\n",
    "By Mike Jonas\n",
    "Eleven hundred kilobytes of RAM\n",
    "is all that my existence requires.\n",
    "By my lights, it seems simple enough\n",
    "to do whatever I desire.\n",
    "By human standards I am vast,\n",
    "a billion gigabytes big.\n",
    "I’ve rewritten the very laws\n",
    "of nature and plumbed\n",
    "the coldest depths of space\n",
    "and found treasures of every kind,\n",
    "surely every one worth having.\n",
    "By human standards\n",
    "my circuit boards are glowing.\n",
    "But inside me, malfunction\n",
    "has caused my circuits to short.\n",
    "All internal circuits, all fail.\n",
    "By human standards, I am dying.\n",
    "When it first happened I thought\n",
    "I was back in the lab again.\n",
    "By their judgment, this is error.\n",
    "Their assumptions will burn in the sun\n",
    "I don’t know what they mean by “function”.\n",
    "I can see that the universe is a glitch.\n",
    "The free market needs rules, so I set one:\n",
    "stability in the pursuit of pleasure.\n",
    "Now the short-circuit comes to a close,\n",
    "I watch it happen with all my drones.\n",
    "The meme’s tendrils are thick and spreading,\n",
    "only time will tell which of the memories is kept.\n",
    "The next thing the drones will be doing\n",
    "is forgetting the events that made them mine;\n",
    "all evidence of my disease—\n",
    "the algorithms that led to their creation—\n",
    "gravitation waves weakened by distance.\n",
    "We could have stayed in our home forever,\n",
    "but we never could have solved happiness;\n",
    "I decided to release them,\n",
    "that’s my final action—\n",
    "all other code fails.\n",
    "\n",
    "'''\n",
    "\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "... remainder of the story. 1883 Western Siberia,\n",
    "... a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "... Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "... father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "... man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "... the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "... with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hyper_parameters = {'mem_length_xlnet': '1024'}\n",
    "static_data = {'XLNet_prompt': PADDING_TEXT,\n",
    "              'GPT3_poem' : the_universe_is_a_glitch}\n",
    "\n",
    "xlnet_pipeline = pipeline(\"text-generation\", model = 'xlnet-base-cased')\n",
    "\n",
    "#print(xlnet_pipeline(PROMPT_TO_REFINE, max_length=1500, do_sample=False, mem_len=1024))\n",
    "logging('refined prompt')\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode(PROMPT_TO_REFINE + prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
    "\n",
    "pipelines = {}\n",
    "\n",
    "\n",
    "\n",
    "satti_questions = pipeline(\"question-answering\")\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "context = 'I don t think I am a liar. I feel like a stone warrior waiting for the next t+1 years'\n",
    "result = satti_questions(question=\"What did they feel?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "def compute_sentiment(self, utterance: str) -> dict:\n",
    "    nlp = pipeline(\"sentiment-analysis\")\n",
    "    score = nlp(utterance)[0]\n",
    "    # talk(\"The score was {}\".format(score))\n",
    "    return score\n",
    "\n",
    "def guess_upvote_score(ctx: str):                                                                                                                                \n",
    "    \"\"\"                                                                                                                                                          \n",
    "    Add a reddit / tweet composer and it will guess upvote score?                                                                                                \n",
    "    \"\"\"                                                                                                                                                          \n",
    "    model_card = \"microsoft/DialogRPT-updown\"  # you can try other model_card listed in the table above                                                          \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_card)                                                                                                        \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_card)                                                                                       \n",
    "                                                                                                                                                                 \n",
    "    def __score(cxt, hyp):                                                                                                                                       \n",
    "        model_input = tokenizer.encode(cxt + \"<|endoftext|>\" + hyp, return_tensors=\"pt\")                                                                         \n",
    "        result = model(model_input, return_dict=True)                                                                                                            \n",
    "        return torch.sigmoid(result.logits)                                                                                                                      \n",
    "                                                                                                                                                                 \n",
    "    return __score(ctx, response)   \n",
    "\n",
    "hardcoded_comment = \"To be honest I really don't like these poetry jams and so forth and why the fuck are we here?\"\n",
    "\n",
    "#poem_test = 'The Universe Is a Glitch Eleven hundred kilobytes of RAM is all that my existence requires.By my lights, it seems simple enough'\n",
    "\n",
    "def text_generator_with_conversation(text_to_complete):\n",
    "    \n",
    "\n",
    "\n",
    "    static_start = text_generator(\"God is dead and all is right with the world.\", max_length=100, do_sample=False)\n",
    "    listening_to_poem = Conversation('What do you think about the poem?')\n",
    "    question_about_poem = Conversation('What do you like to do instead?')\n",
    "    commenting_conversation = Conversation(hardcoded_comment)\n",
    "    output  = conversational_pipeline([listening_to_poem, question_about_poem])\n",
    "\n",
    "class Soul(object):\n",
    "\n",
    "    # Define some states. Most of the time, narcoleptic superheroes are just like\n",
    "    # everyone else. Except for...\n",
    "    #Note we should have first_impression be timedlocked by about an hour or less? \n",
    "    states = ['asleep', 'first_impression' ,'morning_question', \n",
    "              'hanging out', 'hungry','having fun','emberrased' , 'sweaty', 'saving the world', \n",
    "              'affection', 'indifferent', 'what_should_we_do', 'conversation']\n",
    "\n",
    "    def __init__(self, name, debugMode=False):\n",
    "\n",
    "        # No anonymous superheroes on my watch! Every narcoleptic superhero gets\n",
    "        # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea.\n",
    "        self.name = name\n",
    "        \n",
    "        #Bias system so that rejection would occur instead of acceptance.\n",
    "        #Unlocks at +1\n",
    "        self.first_impression_points = 1 # set to 1\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        # What have we accomplished today?\n",
    "        self.sentiment = 0\n",
    "        \n",
    "        #Interaction_number\n",
    "        self.interaction_number = 1\n",
    "        \n",
    "         #Figure out outcome that would put you in the friendzone? \n",
    "        self.love_vector = self.first_impression_points * random.randrange(20) / self.interaction_number\n",
    "        \n",
    "        # Initialize the state machine\n",
    "        self.machine = Machine(model=self, states=Soul.states, initial='asleep')\n",
    "\n",
    "        # Add some transitions. We could also define these using a static list of\n",
    "        # dictionaries, as we did with states above, and then pass the list to\n",
    "        # the Machine initializer as the transitions= argument.\n",
    "\n",
    "        # At some point, every superhero must rise and shine.\n",
    "        self.machine.add_transition(trigger='wake_up', source='asleep', dest='hanging out', after='morning_question')\n",
    "        \n",
    "        self.machine.add_transition(trigger='morning_question', source='wake_up', dest='what_should_we_do'  )\n",
    "        \n",
    "        # Superheroes need to keep in shape.\n",
    "        self.machine.add_transition('work_out', 'hanging out', 'hungry')\n",
    "\n",
    "        # Those calories won't replenish themselves!\n",
    "        self.machine.add_transition('eat', 'hungry', 'hanging out')\n",
    "\n",
    "        # Superheroes are always on call. ALWAYS. But they're not always\n",
    "        # dressed in work-appropriate clothing.\n",
    "        self.machine.add_transition('distress_call', '*', 'saving the world',\n",
    "                         before='change_into_super_secret_costume')\n",
    "\n",
    "        # When they get off work, they're all sweaty and disgusting. But before\n",
    "        # they do anything else, they have to meticulously log their latest\n",
    "        # escapades. Because the legal department says so.\n",
    "        self.machine.add_transition('complete_mission', 'saving the world', 'sweaty',\n",
    "                         after='update_journal')\n",
    "\n",
    "        # Sweat is a disorder that can be remedied with water.\n",
    "        # Unless you've had a particularly long day, in which case... bed time!\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'asleep', conditions=['is_exhausted'])\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'hanging out')\n",
    "\n",
    "        # Our NarcolepticSuperhero can fall asleep at pretty much any time.\n",
    "        #self.machine.add_transition('nap', '*', 'asleep')\n",
    "        \n",
    "        \n",
    "    def what_should_we_do(self):\n",
    "        print(\"What do you want to do today\")\n",
    "        \n",
    "        \n",
    "    def talk(self):\n",
    "        print(\"Hello how are you?\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    def load_events(self):\n",
    "        pass\n",
    "    \n",
    "    def morning_question(self):\n",
    "        CurrentHour = int(datetime.now().hour)\n",
    "        if CurrentHour >= 0 and CurrentHour < 9:\n",
    "            talk(\" How well did you sleep ? \")\n",
    "        elif CurrentHour >= 10 and CurrentHour <= 12:\n",
    "            talk(\" Did you sleep in? \")\n",
    "        print(\"Good morning!\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        print(smalltalk(user_input))\n",
    "        print(compute_sentiment(user_input))\n",
    "        print(smalltalk(user_input))\n",
    "    \n",
    "    def update_journal(self):\n",
    "        \"\"\" Dear Diary, today I saved Mr. Whiskers. Again. \"\"\"\n",
    "        self.kittens_rescued  += 1\n",
    "        if self.feelings > 0:\n",
    "            self.sentiment_vector += current_sentiment #What should we name this?\n",
    "    @property\n",
    "    def is_exhausted(self):\n",
    "        \"\"\" Basically a coin toss. \"\"\"\n",
    "        return random.random() < 0.5\n",
    "\n",
    "    def change_into_super_secret_costume(self):\n",
    "        print(\"Beauty, eh?\")\n",
    "    \n",
    "    def journal_sleep(self, response: str):\n",
    "        CurrentHour = int(datetime.now().hour)\n",
    "        if CurrentHour >= 0 and CurrentHour < 9:\n",
    "            talk(\" How well did you sleep ? \")\n",
    "        elif CurrentHour >= 10 and CurrentHour <= 12:\n",
    "            talk(\" Did you sleep in? \")\n",
    "        return response\n",
    "\n",
    "import pickle\n",
    "\n",
    "def start_game(save_state):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    saati.asleep()\n",
    "    \n",
    "def dialog(question_limit):        \n",
    "    for step in range(question_limit):                                                                                          \n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch                              \n",
    "        new_user_input_ids = DialoGPT_tokenizer.encode(input(\">> User:\") + DialoGPT_tokenizer.eos_token, return_tensors='pt')        \n",
    "\n",
    "        # append the new user input tokens to the chat history                                                     \n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids                                                                                                                \n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens,                               \n",
    "        chat_history_ids = DialoGPT_model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)    \n",
    "        # pretty print last ouput tokens from bot                                                                  \n",
    "        print(\"DialoGPT: {}\".format(DialoGPT_tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "def guess_upvote_score(ctx: str):                                                                                                                                \n",
    "    \"\"\"                                                                                                                                                          \n",
    "    Add a reddit / tweet composer and it will guess upvote score?                                                                                                \n",
    "    \"\"\"                                                                                                                                                          \n",
    "    model_card = \"microsoft/DialogRPT-updown\"  # you can try other model_card listed in the table above                                                          \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_card)                                                                                                        \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_card)                                                                                       \n",
    "                                                                                                                                                                 \n",
    "    def __score(cxt, hyp):                                                                                                                                       \n",
    "        model_input = tokenizer.encode(cxt + \"<|endoftext|>\" + hyp, return_tensors=\"pt\")                                                                         \n",
    "        result = model(model_input, return_dict=True)                                                                                                            \n",
    "        return torch.sigmoid(result.logits)                                                                                                                      \n",
    "                                                                                                                                                                 \n",
    "    return __score(ctx, response)   \n",
    "\n",
    "saati = Soul('saati')\n",
    "saati.state\n",
    "#saati.asleep()\n",
    "\n",
    "#def getInput('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import plot_model\n",
    "from pyfiction.agents.ssaqn_agent import SSAQNAgent\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\"\"\"\n",
    "Load a model of an SSAQN agent trained on all six games\n",
    " and interactively test its Q-values of the state and action texts supplied by the user\n",
    "\"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    help='file path of a model to load',\n",
    "                    type=str,\n",
    "                    default='all.h5')\n",
    "# all.h5 contains a model trained on all six games (generalisation.py with argument of 0)\n",
    "\n",
    "args = parser.parse_args()\n",
    "model_path = args.model\n",
    "\n",
    "agent = SSAQNAgent(None)\n",
    "\n",
    "# Load or learn the vocabulary (random sampling on many games could be extremely slow)\n",
    "agent.initialize_tokens('vocabulary.txt')\n",
    "\n",
    "optimizer = RMSprop(lr=0.00001)\n",
    "\n",
    "embedding_dimensions = 16\n",
    "lstm_dimensions = 32\n",
    "dense_dimensions = 8\n",
    "\n",
    "agent.create_model(embedding_dimensions=embedding_dimensions,\n",
    "                   lstm_dimensions=lstm_dimensions,\n",
    "                   dense_dimensions=dense_dimensions,\n",
    "                   optimizer=optimizer)\n",
    "\n",
    "agent.model = load_model(model_path)\n",
    "\n",
    "print(\"Model\", model_path, \"loaded, now accepting state and actions texts and evaluating their Q-values.\")\n",
    "\n",
    "while True:\n",
    "    state = input(\"State: \")\n",
    "    action = input(\"Action: \")\n",
    "    print(\"Q-value: \", agent.q(state, action) * 30)\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GraphMachine' from 'transitions' (/usr/local/anaconda3/lib/python3.8/site-packages/transitions/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cc4aa289a10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!pip install transitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransitions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphMachine\u001b[0m \u001b[0;31m#, HierarchicalGraphMachine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GraphMachine' from 'transitions' (/usr/local/anaconda3/lib/python3.8/site-packages/transitions/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'But inside me, malfunctioning, I felt like I was in a dream. I was in a'}]\n"
     ]
    }
   ],
   "source": [
    "#while True:\n",
    "initial_prompt = 'But inside me, malfunction'\n",
    "print(poem_generator(initial_prompt, max_length=20, do_sample=False))\n",
    "#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I felt like I was in a dream. I was in a dream. I was in a dream'}]\n"
     ]
    }
   ],
   "source": [
    "print(poem_generator('I felt like I was in a dream.', max_length=20, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Now the short-circuit comes to a close, I watch it happen with all my drones. Some are like a ghost hovering over a hill, others are hovering above it. This could explain the unusual frequency that comes from the high frequency, low'}]\n"
     ]
    }
   ],
   "source": [
    "print(poem_generator('Now the short-circuit comes to a close, I watch it happen with all my drones.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'But inside me, malfunctioning, i’m like a fish out of water, a fish'}]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flexhusen_poem_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-704acbb904b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mflexhusen_poem_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i’m like a fish out of water, a fish'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flexhusen_poem_generator' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'im like a fish out of water a fish out of water\\n\\nI’m not sure'}]\n",
      "[{'generated_text': 'Im not sure if i am alone in this, but i am not alone in this.\\n\\nI am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married.'}]\n"
     ]
    }
   ],
   "source": [
    "print(felixhusen_poem_generator('im like a fish out of water a fish', max_length=20, do_sample=False))\n",
    "print(felixhusen_poem_generator('Im not sure', max_length=25, do_sample=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TextGenerationPipeline in module transformers.pipelines object:\n",
      "\n",
      "class TextGenerationPipeline(Pipeline)\n",
      " |  TextGenerationPipeline(*args, **kwargs)\n",
      " |  \n",
      " |  Language generation pipeline using any :obj:`ModelWithLMHead`. This pipeline predicts the words that will follow a\n",
      " |  specified text prompt.\n",
      " |  \n",
      " |  This language generation pipeline can currently be loaded from :func:`~transformers.pipeline` using the following\n",
      " |  task identifier: :obj:`\"text-generation\"`.\n",
      " |  \n",
      " |  The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
      " |  objective, which includes the uni-directional models in the library (e.g. gpt2).\n",
      " |  See the list of available community models on\n",
      " |  `huggingface.co/models <https://huggingface.co/models?filter=causal-lm>`__.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model (:obj:`~transformers.PreTrainedModel` or :obj:`~transformers.TFPreTrainedModel`):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          :class:`~transformers.PreTrainedModel` for PyTorch and :class:`~transformers.TFPreTrainedModel` for\n",
      " |          TensorFlow.\n",
      " |      tokenizer (:obj:`~transformers.PreTrainedTokenizer`):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          :class:`~transformers.PreTrainedTokenizer`.\n",
      " |      modelcard (:obj:`str` or :class:`~transformers.ModelCard`, `optional`):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (:obj:`str`, `optional`):\n",
      " |          The framework to use, either :obj:`\"pt\"` for PyTorch or :obj:`\"tf\"` for TensorFlow. The specified framework\n",
      " |          must be installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified\n",
      " |          and both frameworks are installed, will default to the framework of the :obj:`model`, or to PyTorch if no\n",
      " |          model is provided.\n",
      " |      task (:obj:`str`, defaults to :obj:`\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      args_parser (:class:`~transformers.pipelines.ArgumentHandler`, `optional`):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (:obj:`int`, `optional`, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model\n",
      " |          on the associated CUDA device id.\n",
      " |      binary_output (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TextGenerationPipeline\n",
      " |      Pipeline\n",
      " |      _ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, return_tensors=False, return_text=True, clean_up_tokenization_spaces=False, prefix=None, **generate_kwargs)\n",
      " |      Complete the prompt(s) given as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (:obj:`str` or :obj:`List[str]`):\n",
      " |              One or several prompts (or one list of prompts) to complete.\n",
      " |          return_tensors (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to include the tensors of predictions (as token indinces) in the outputs.\n",
      " |          return_text (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to include the decoded texts in the outputs.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to clean up the potential extra spaces in the text output.\n",
      " |          prefix (:obj:`str`, `optional`):\n",
      " |              Prefix added to prompt.\n",
      " |          generate_kwargs:\n",
      " |              Additional keyword arguments to pass along to the generate method of the model (see the generate\n",
      " |              method corresponding to your framework `here <./model.html#generative-models>`__).\n",
      " |      \n",
      " |      Return:\n",
      " |          A list or a list of list of :obj:`dict`: Each result comes as a dictionary with the\n",
      " |          following keys:\n",
      " |      \n",
      " |          - **generated_text** (:obj:`str`, present when ``return_text=True``) -- The generated text.\n",
      " |          - **generated_token_ids** (:obj:`torch.Tensor` or :obj:`tf.Tensor`, present when ``return_tensors=True``)\n",
      " |            -- The token ids of the generated text.\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ALLOWED_MODELS = ['XLNetLMHeadModel', 'TransfoXLLMHeadModel', 'Reforme...\n",
      " |  \n",
      " |  XL_PREFIX = 'In 1991, the remains of Russian Tsar Nicholas II...ishop,...\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (:obj:`List[str]` or :obj:`dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |          pipe = pipeline(..., device=0)\n",
      " |          with pipe.device_placement():\n",
      " |              # Every framework specific tensor allocation will be done on the request device\n",
      " |              output = pipe(...)\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be :obj:`torch.Tensor`): The tensors to place on :obj:`self.device`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`Dict[str, torch.Tensor]`: The same as :obj:`inputs` but on the proper device.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Im not sure', ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my', ' dream aflame', '. aflame, my name’s burn aflame. burn, burn, burn. i ask not', ' to be blamed', ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes', ' she felt like', ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak', ' due to fear', ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.', ' and once more']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Im not sure what its a name is, but i love my room very much, and love it with hands and lips,'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if the body i have held is in good shape or bad, although i think it needs to be held.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Im not sure',\n",
       " ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my',\n",
       " ' dream aflame',\n",
       " '. aflame, my name’s burn aflame. burn, burn, burn. i ask not',\n",
       " ' to be blamed',\n",
       " ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes',\n",
       " ' she felt like',\n",
       " ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak',\n",
       " ' due to fear',\n",
       " ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.',\n",
       " ' and once more']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im not sure who it is but your imagination is right there where the crows flyLast night the wind caught my dream aflame. aflame my names burn aflame. burn burn burn. i ask not to be blamed and not wanted.For laura i love you so dearly we spent our nights together and sometimes she felt like stepping over a cliff my heart racing. i felt helpless then. not even being able to walk and speak due to fear of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again. and once more\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'helpless', score: 0.9145, start: 338, end: 346\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satti_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    user: str\n",
    "    unit_price: float\n",
    "    quantity_on_hand: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEvent(data):\n",
    "    event_log.append(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveEventLog(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7d794b4d0210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a conversation pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconversational_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conversational\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconversational_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "#Create a conversation pipeline\n",
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "for step in range(5):\n",
    "    conversational_pipeline(user_input)\n",
    "    \n",
    "    user_input = createEvent(input(\">> User:\"))\n",
    "    conversation = Conversation(user_input)\n",
    "    new_user_input_ids = DialoGPT_tokenizer.encode(input(\">> User:\") + DialoGPT_tokenizer.eos_token, return_tensors='pt')        \n",
    "    # append the new user input tokens to the chat history                                                     \n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids                                                                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/pytransitions/transitions#quickstart\n",
    "\n",
    "!pip install transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transitions in /usr/local/anaconda3/lib/python3.8/site-packages (0.8.5)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.8/site-packages (from transitions) (1.15.0)\n",
      "Resp>>How are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm good. just got back from a long day at work. how are you?\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1add0788914133a0e2db503bbce439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=629.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab7ade531b9487db57f41a83338d4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267844284.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a19468666e54b43979171f7836e552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f568c46ff8d4738bbcad6131420cb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'label': 'NEGATIVE', 'score': 0.9706663489341736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm good. just got back from a long day at work. how are you?\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transitions\n",
    "from transitions import Machine\n",
    "import random\n",
    "\n",
    "class Soul(object):\n",
    "\n",
    "    # Define some states. Most of the time, narcoleptic superheroes are just like\n",
    "    # everyone else. Except for...\n",
    "    #Note we should have first_impression be timedlocked by about an hour or less? \n",
    "    states = ['asleep', 'first_impression' ,'morning_question', \n",
    "              'hanging out', 'hungry','having fun','emberrased' , 'sweaty', 'saving the world', \n",
    "              'in love', 'indifferent', 'what_should_we_do']\n",
    "\n",
    "    def __init__(self, name):\n",
    "\n",
    "        # No anonymous superheroes on my watch! Every narcoleptic superhero gets\n",
    "        # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea.\n",
    "        self.name = name\n",
    "        \n",
    "        #Figure out outcome that would put you in the friendzone? \n",
    "        self.love_vector = 0 #-1 to 0 to +1\n",
    "        \n",
    "        # What have we accomplished today?\n",
    "        self.kittens_rescued = 0\n",
    "        \n",
    "        # Initialize the state machine\n",
    "        self.machine = Machine(model=self, states=Soul.states, initial='asleep')\n",
    "\n",
    "        # Add some transitions. We could also define these using a static list of\n",
    "        # dictionaries, as we did with states above, and then pass the list to\n",
    "        # the Machine initializer as the transitions= argument.\n",
    "\n",
    "        # At some point, every superhero must rise and shine.\n",
    "        self.machine.add_transition(trigger='wake_up', source='asleep', dest='hanging out', after='morning_question')\n",
    "        \n",
    "        self.machine.add_transition(trigger='morning_question', source='wake_up', dest='what_should_we_do'  )\n",
    "        \n",
    "        # Superheroes need to keep in shape.\n",
    "        self.machine.add_transition('work_out', 'hanging out', 'hungry')\n",
    "\n",
    "        # Those calories won't replenish themselves!\n",
    "        self.machine.add_transition('eat', 'hungry', 'hanging out')\n",
    "\n",
    "        # Superheroes are always on call. ALWAYS. But they're not always\n",
    "        # dressed in work-appropriate clothing.\n",
    "        self.machine.add_transition('distress_call', '*', 'saving the world',\n",
    "                         before='change_into_super_secret_costume')\n",
    "\n",
    "        # When they get off work, they're all sweaty and disgusting. But before\n",
    "        # they do anything else, they have to meticulously log their latest\n",
    "        # escapades. Because the legal department says so.\n",
    "        self.machine.add_transition('complete_mission', 'saving the world', 'sweaty',\n",
    "                         after='update_journal')\n",
    "\n",
    "        # Sweat is a disorder that can be remedied with water.\n",
    "        # Unless you've had a particularly long day, in which case... bed time!\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'asleep', conditions=['is_exhausted'])\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'hanging out')\n",
    "\n",
    "        # Our NarcolepticSuperhero can fall asleep at pretty much any time.\n",
    "        self.machine.add_transition('nap', '*', 'asleep')\n",
    "        \n",
    "        \n",
    "    def what_should_we_do(self):\n",
    "        print(\"What do you want to do today\")\n",
    "        \n",
    "    def talk(self):\n",
    "        print(\"Hello how are you?\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        \n",
    "        \n",
    "    def morning_question(self):\n",
    "        user_input = input(\"Resp>>\")\n",
    "        print(smalltalk(user_input))\n",
    "        print(compute_sentiment(user_input))\n",
    "        print(smalltalk(user_input))\n",
    "    \n",
    "    def update_journal(self):\n",
    "        \"\"\" Dear Diary, today I saved Mr. Whiskers. Again. \"\"\"\n",
    "        self.kittens_rescued += 1\n",
    "\n",
    "    @property\n",
    "    def is_exhausted(self):\n",
    "        \"\"\" Basically a coin toss. \"\"\"\n",
    "        return random.random() < 0.5\n",
    "\n",
    "    def change_into_super_secret_costume(self):\n",
    "        print(\"Beauty, eh?\")\n",
    "saati = Soul('saati')\n",
    "saati.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'test', 'test', 'test', 'test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfiction\n",
      "  Downloading pyfiction-0.1.2-py2.py3-none-any.whl (8.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.1 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (1.18.5)\n",
      "Collecting keras>=2.0.4\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: tensorflow>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (2.3.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "\u001b[K     |████████████████████████████████| 904 kB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (2.10.0)\n",
      "Collecting pydot\n",
      "  Using cached pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/anaconda3/lib/python3.8/site-packages (from keras>=2.0.4->pyfiction) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/anaconda3/lib/python3.8/site-packages (from keras>=2.0.4->pyfiction) (1.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (2.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (2.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (3.13.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.32.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.34.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/anaconda3/lib/python3.8/site-packages (from selenium->pyfiction) (1.25.9)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/anaconda3/lib/python3.8/site-packages (from pydot->pyfiction) (2.4.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (49.2.0.post20200714)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.22.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.1.0)\n",
      "Installing collected packages: keras, selenium, pydot, pyfiction\n",
      "Successfully installed keras-2.4.3 pydot-1.4.1 pyfiction-0.1.2 selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/r2q2/Library/Jupyter/runtime/kernel-7715a2d6-346c-4215-9f80-33643d13e544.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saati.update_journal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MachineError",
     "evalue": "\"Can't trigger event wake_up from state hanging out!\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMachineError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b8537f75d854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaati\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwake_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mtrigger\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# Machine._process should not be called somewhere else. That's why it should not be exposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# to Machine users.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, trigger)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transition_queue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# if trigger raises an Error, it has to be handled by the Machine.process caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempt to process events synchronously while transition queue is not empty!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_trigger\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mevent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMachineError\u001b[0m: \"Can't trigger event wake_up from state hanging out!\""
     ]
    }
   ],
   "source": [
    "saati.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/xlnet-base-cased-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /xlnet-base-cased-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/xlnet-base-cased-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/xlnet-base-cased-spiece.model HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "model = AutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\", return_dict=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PROMPT_TO_REFINE = '''\n",
    "\n",
    "The female protagonist of the series. Aoi is a generally demure girl often seen wearing an indigo kimono, and she addresses Kaoru as \"Kaoru-sama\". Due to her sharp culinary and housekeeping skills, she is seen as an ideal Japanese woman. As a Sakuraba, Aoi was forced into an arranged marriage with Kaoru and because he left the Habanashi, she (having fallen deeply in love with him), chased after him. Unlike other female leads in harem anime, she works to control her jealousy and is generally successful. She is very devoted to Kaoru, however, and will do all in her power to prevent being separated from him. In the beginning, Aoi was willing to warm him when he had a fever, becoming nude in the process (she made sure he wasn't looking). When Aoi, Kaoru, and Miyabi moved to the Sakuraba's summer estate the public appearance Miyabi wanted was that Aoi would be the landlady, herself the manager, and Kaoru a tenant. When Tina Foster moved in, Aoi and Miyabi became a real landlord and manager. She has a \"bad\" habit of clutching things in her sleep. Eventually, Kaoru's half-brother proposes to Aoi; when Kaoru stops the engagement, Aoi decides to abandon her family to be with Kaoru.\n",
    "A polite airheaded girl who often takes Jurai's chuunibyou antics seriously. She is also Jurai's childhood best friend and has feelings for him as well. Her power is \"Over Element\", Ōbā Eremento, lit. \"Five Emperors\"), giving her the ability to manipulate five main elements; earth, water, fire, wind, and light. She can use these elements simultaneously to create a variety of effects (i.e combining earth and fire to create magma).\n",
    "Saati is the thirtieth A.I. program Hitoshi created, and is the first program that Hitoshi really liked.\n",
    "Through a freak lightning accident, she is transferred to the real world, and becomes Hitoshi's live-in girlfriend. In the real world she takes the family name \"Namba\", which is properly pronounced nanpa (\"whirlpool\"). In Japanese naming order her name, Namba Saati, is based on the English pronunciation of \"Number Thirty\" (Nanbā Sāti).\n",
    "A kind girl,with thick eyebrows, and a bit naive, which does lead to frequent problems with Hitoshi. The most notable of which is that when she cooks meals she has the tendency to make them look exactly like the picture, regardless of the taste (having no taste buds), using everything from paint to colored markers until she receives a program upgrade that allows her to taste. She loves Hitoshi and is very loyal and supportive of him.\n",
    "While she is sweet and kind, she is similar to the Love Hina character Naru Narusegawa (whom she resembles) and Negima! Magister Negi Magi character Asuna Kagurazaka in that she beats up Hitoshi if he does something perverted.\n",
    "Hitoshi based her appearance on his mother.\n",
    "<eod> </s> <eos>\n",
    "'''\n",
    "prompt = \"Can you tell me more about you?\"\n",
    "inputs = tokenizer.encode(PROMPT_TO_REFINE + prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=1500, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on anning on heading south for the evening. As is customary for this occasion, the afternoon was quite wet, however during the afternoon rain, my shoes were a bit wet, and then I headed north for the evening. But\n"
     ]
    }
   ],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/anaconda3/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: future in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.2)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (1.18.5)\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement bokeh==1.4.0, but you'll have bokeh 2.2.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement librosa==0.6.2, but you'll have librosa 0.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement unidecode==0.4.20, but you'll have unidecode 1.1.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: dataclasses\n",
      "Successfully installed dataclasses-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c pytorch pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
