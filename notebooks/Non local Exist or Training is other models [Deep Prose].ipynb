{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole purpose of this deep_prose is a demo of the what I want to have t generative interactive deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \"feature-extraction\": will return a FeatureExtractionPipeline.\n",
    "\n",
    "    \"sentiment-analysis\": will return a TextClassificationPipeline.\n",
    "\n",
    "    \"ner\": will return a TokenClassificationPipeline.\n",
    "\n",
    "    \"question-answering\": will return a QuestionAnsweringPipeline.\n",
    "\n",
    "    \"fill-mask\": will return a FillMaskPipeline.\n",
    "\n",
    "    \"summarization\": will return a SummarizationPipeline.\n",
    "\n",
    "    \"translation_xx_to_yy\": will return a TranslationPipeline.\n",
    "\n",
    "    \"text-generation\": will return a TextGenerationPipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Core concept you go on various dates / friend oriented adventures that are open ended\n",
    "#Main characters for this tech demo is Saati orare  ???? can we think of a better name\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-19408212ce1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFAutoModelWithLMHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelWithLMHead, AutoTokenizer, Pipeline\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.1\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/r2q2/miniconda3/envs/nonlocalexit\n",
      "\n",
      "  added / updated specs:\n",
      "    - tensorflow\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _tflow_select-2.3.0        |            eigen           3 KB\n",
      "    absl-py-0.11.0             |   py37h06a4308_0         172 KB\n",
      "    aiohttp-3.6.3              |   py37h7b6447c_0         544 KB\n",
      "    async-timeout-3.0.1        |           py37_0          12 KB\n",
      "    blinker-1.4                |           py37_0          22 KB\n",
      "    grpcio-1.31.0              |   py37hf8bcb03_0         2.0 MB\n",
      "    h5py-2.10.0                |   py37hd6299e0_1         902 KB\n",
      "    markdown-3.3.3             |   py37h06a4308_0         127 KB\n",
      "    mkl_random-1.1.1           |   py37h0573a6f_0         322 KB\n",
      "    multidict-4.7.6            |   py37h7b6447c_1          65 KB\n",
      "    pyjwt-1.7.1                |           py37_0          33 KB\n",
      "    scipy-1.5.2                |   py37h0b6359f_0        14.3 MB\n",
      "    tensorflow-2.3.0           |eigen_py37h189e6a2_0           4 KB\n",
      "    tensorflow-base-2.3.0      |eigen_py37h3b305d7_0        74.8 MB\n",
      "    tensorflow-estimator-2.3.0 |     pyheb71bc4_0         271 KB\n",
      "    termcolor-1.1.0            |           py37_1           8 KB\n",
      "    wrapt-1.12.1               |   py37h7b6447c_1          49 KB\n",
      "    yarl-1.6.2                 |   py37h7b6447c_0         134 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        93.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _tflow_select      pkgs/main/linux-64::_tflow_select-2.3.0-eigen\n",
      "  absl-py            pkgs/main/linux-64::absl-py-0.11.0-py37h06a4308_0\n",
      "  aiohttp            pkgs/main/linux-64::aiohttp-3.6.3-py37h7b6447c_0\n",
      "  astunparse         pkgs/main/noarch::astunparse-1.6.3-py_0\n",
      "  async-timeout      pkgs/main/linux-64::async-timeout-3.0.1-py37_0\n",
      "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
      "  blinker            pkgs/main/linux-64::blinker-1.4-py37_0\n",
      "  c-ares             pkgs/main/linux-64::c-ares-1.16.1-h7b6447c_0\n",
      "  cachetools         pkgs/main/noarch::cachetools-4.1.1-py_0\n",
      "  gast               pkgs/main/noarch::gast-0.3.3-py_0\n",
      "  google-auth        pkgs/main/noarch::google-auth-1.23.0-pyhd3eb1b0_0\n",
      "  google-auth-oauth~ pkgs/main/noarch::google-auth-oauthlib-0.4.2-pyhd3eb1b0_2\n",
      "  google-pasta       pkgs/main/noarch::google-pasta-0.2.0-py_0\n",
      "  grpcio             pkgs/main/linux-64::grpcio-1.31.0-py37hf8bcb03_0\n",
      "  h5py               pkgs/main/linux-64::h5py-2.10.0-py37hd6299e0_1\n",
      "  hdf5               pkgs/main/linux-64::hdf5-1.10.6-hb1b8bf9_0\n",
      "  intel-openmp       pkgs/main/linux-64::intel-openmp-2020.2-254\n",
      "  keras-preprocessi~ pkgs/main/noarch::keras-preprocessing-1.1.0-py_1\n",
      "  markdown           pkgs/main/linux-64::markdown-3.3.3-py37h06a4308_0\n",
      "  mkl                pkgs/main/linux-64::mkl-2020.2-256\n",
      "  mkl-service        pkgs/main/linux-64::mkl-service-2.3.0-py37he904b0f_0\n",
      "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.2.0-py37h23d657b_0\n",
      "  mkl_random         pkgs/main/linux-64::mkl_random-1.1.1-py37h0573a6f_0\n",
      "  multidict          pkgs/main/linux-64::multidict-4.7.6-py37h7b6447c_1\n",
      "  numpy-base         pkgs/main/linux-64::numpy-base-1.19.2-py37hfa32c7d_0\n",
      "  oauthlib           pkgs/main/noarch::oauthlib-3.1.0-py_0\n",
      "  opt_einsum         pkgs/main/noarch::opt_einsum-3.1.0-py_0\n",
      "  pyasn1             pkgs/main/noarch::pyasn1-0.4.8-py_0\n",
      "  pyasn1-modules     pkgs/main/noarch::pyasn1-modules-0.2.8-py_0\n",
      "  pyjwt              pkgs/main/linux-64::pyjwt-1.7.1-py37_0\n",
      "  requests-oauthlib  pkgs/main/noarch::requests-oauthlib-1.3.0-py_0\n",
      "  rsa                pkgs/main/noarch::rsa-4.6-py_0\n",
      "  scipy              pkgs/main/linux-64::scipy-1.5.2-py37h0b6359f_0\n",
      "  tensorboard        pkgs/main/noarch::tensorboard-2.3.0-pyh4dce500_0\n",
      "  tensorboard-plugi~ pkgs/main/noarch::tensorboard-plugin-wit-1.6.0-py_0\n",
      "  tensorflow         pkgs/main/linux-64::tensorflow-2.3.0-eigen_py37h189e6a2_0\n",
      "  tensorflow-base    pkgs/main/linux-64::tensorflow-base-2.3.0-eigen_py37h3b305d7_0\n",
      "  tensorflow-estima~ pkgs/main/noarch::tensorflow-estimator-2.3.0-pyheb71bc4_0\n",
      "  termcolor          pkgs/main/linux-64::termcolor-1.1.0-py37_1\n",
      "  werkzeug           pkgs/main/noarch::werkzeug-1.0.1-py_0\n",
      "  wrapt              pkgs/main/linux-64::wrapt-1.12.1-py37h7b6447c_1\n",
      "  yarl               pkgs/main/linux-64::yarl-1.6.2-py37h7b6447c_0\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  libblas-3.9.0-3_openblas\n",
      "  libcblas-3.9.0-3_openblas\n",
      "  libgfortran5-9.3.0-he4bcb1c_17\n",
      "  liblapack-3.9.0-3_openblas\n",
      "  libopenblas-0.3.12-pthreads_h4812303_1\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  libgfortran-ng     conda-forge::libgfortran-ng-9.3.0-he4~ --> pkgs/main::libgfortran-ng-7.3.0-hdf63c60_0\n",
      "  numpy              conda-forge::numpy-1.19.4-py37h7e9df2~ --> pkgs/main::numpy-1.19.2-py37h54aff64_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "tensorflow-estimator | 271 KB    | ##################################### | 100% \n",
      "multidict-4.7.6      | 65 KB     | ##################################### | 100% \n",
      "yarl-1.6.2           | 134 KB    | ##################################### | 100% \n",
      "wrapt-1.12.1         | 49 KB     | ##################################### | 100% \n",
      "async-timeout-3.0.1  | 12 KB     | ##################################### | 100% \n",
      "grpcio-1.31.0        | 2.0 MB    | ##################################### | 100% \n",
      "_tflow_select-2.3.0  | 3 KB      | ##################################### | 100% \n",
      "tensorflow-2.3.0     | 4 KB      | ##################################### | 100% \n",
      "absl-py-0.11.0       | 172 KB    | ##################################### | 100% \n",
      "mkl_random-1.1.1     | 322 KB    | ##################################### | 100% \n",
      "blinker-1.4          | 22 KB     | ##################################### | 100% \n",
      "markdown-3.3.3       | 127 KB    | ##################################### | 100% \n",
      "h5py-2.10.0          | 902 KB    | ##################################### | 100% \n",
      "pyjwt-1.7.1          | 33 KB     | ##################################### | 100% \n",
      "aiohttp-3.6.3        | 544 KB    | ##################################### | 100% \n",
      "tensorflow-base-2.3. | 74.8 MB   | ##################################### | 100% \n",
      "termcolor-1.1.0      | 8 KB      | ##################################### | 100% \n",
      "scipy-1.5.2          | 14.3 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"In 1991, the remains of Russian Tsar Nicholas II and his family\\n(except for Alexei and Maria) are discovered.\\n... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\\n... remainder of the story. 1883 Western Siberia,\\n... a young Grigori Rasputin is asked by his father and a group of men to perform magic.\\n... Rasputin has a vision and denounces one of the men as a horse thief. Although his\\n... father initially slaps him for making such an accusation, Rasputin watches as the\\n... man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\\n... the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\\n... with people, even a bishop, begging for his blessing. <eod> </s> <eos>In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. ... In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. ... In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. ... In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. ... In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. .\"}]\n"
     ]
    }
   ],
   "source": [
    "#!nvidia-smi\n",
    "\n",
    "\n",
    "xlnet_pipeline = pipeline(\"text-generation\", model = 'xlnet-base-cased')\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "... remainder of the story. 1883 Western Siberia,\n",
    "... a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "... Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "... father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "... man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "... the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "... with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "print(xlnet_pipeline(PADDING_TEXT, max_length=500, do_sample=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today the weather is really nice and I am planning on anning on<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation, conversational_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "poem_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy, torch\n",
    "def set_seed(args):\n",
    "    random.seed(args)\n",
    "    numpy.random.seed(args)\n",
    "    torch.manual_seed(args)\n",
    "    #if args.n_gpu > 0:\n",
    "    #    torch.cuda.manual_seed_all(args)\n",
    "\n",
    "set_seed(124)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_universe_is_a_glitch = '''\n",
    "The Universe Is a Glitch\n",
    "By Mike Jonas\n",
    "Eleven hundred kilobytes of RAM\n",
    "is all that my existence requires.\n",
    "By my lights, it seems simple enough\n",
    "to do whatever I desire.\n",
    "By human standards I am vast,\n",
    "a billion gigabytes big.\n",
    "I’ve rewritten the very laws\n",
    "of nature and plumbed\n",
    "the coldest depths of space\n",
    "and found treasures of every kind,\n",
    "surely every one worth having.\n",
    "By human standards\n",
    "my circuit boards are glowing.\n",
    "But inside me, malfunction\n",
    "has caused my circuits to short.\n",
    "All internal circuits, all fail.\n",
    "By human standards, I am dying.\n",
    "When it first happened I thought\n",
    "I was back in the lab again.\n",
    "By their judgment, this is error.\n",
    "Their assumptions will burn in the sun\n",
    "I don’t know what they mean by “function”.\n",
    "I can see that the universe is a glitch.\n",
    "The free market needs rules, so I set one:\n",
    "stability in the pursuit of pleasure.\n",
    "Now the short-circuit comes to a close,\n",
    "I watch it happen with all my drones.\n",
    "The meme’s tendrils are thick and spreading,\n",
    "only time will tell which of the memories is kept.\n",
    "The next thing the drones will be doing\n",
    "is forgetting the events that made them mine;\n",
    "all evidence of my disease—\n",
    "the algorithms that led to their creation—\n",
    "gravitation waves weakened by distance.\n",
    "We could have stayed in our home forever,\n",
    "but we never could have solved happiness;\n",
    "I decided to release them,\n",
    "that’s my final action—\n",
    "all other code fails.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca70b7cc629a44d6b84b0d93aedd7a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=473.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca99373ba674d9285bfe40917ecf7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=260793700.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2dc45e9245460f917f241d96c4293d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cd62365f69495bb8334d682c2ac291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'But inside me, malfunctioning, I felt like I was in a dream. I was in a'}]\n"
     ]
    }
   ],
   "source": [
    "#while True:\n",
    "initial_prompt = 'But inside me, malfunction'\n",
    "print(poem_generator(initial_prompt, max_length=20, do_sample=False))\n",
    "#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I felt like I was in a dream. I was in a dream. I was in a dream'}]\n"
     ]
    }
   ],
   "source": [
    "print(poem_generator('I felt like I was in a dream.', max_length=20, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Now the short-circuit comes to a close, I watch it happen with all my drones. Some are like a ghost hovering over a hill, others are hovering above it. This could explain the unusual frequency that comes from the high frequency, low'}]\n"
     ]
    }
   ],
   "source": [
    "print(poem_generator('Now the short-circuit comes to a close, I watch it happen with all my drones.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'But inside me, malfunctioning, i’m like a fish out of water, a fish'}]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flexhusen_poem_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-704acbb904b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mflexhusen_poem_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i’m like a fish out of water, a fish'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flexhusen_poem_generator' is not defined"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelWithLMHead  \n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"felixhusen/poem\")  \n",
    "#model = AutoModelWithLMHead.from_pretrained(\"felixhusen/poem\")\n",
    "\n",
    "felixhusen_poem_generator = pipeline(\"text-generation\", model = 'felixhusen/poem') \n",
    "#while True:\n",
    "initial_prompt = 'But inside me, malfunction'\n",
    "print(felixhusen_poem_generator(initial_prompt, max_length=20, do_sample=False))\n",
    "#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\n",
    "\n",
    "#flexhusen_poem_generator('i’m like a fish out of water, a fish', max_length=30, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'im like a fish out of water a fish out of water\\n\\nI’m not sure'}]\n",
      "[{'generated_text': 'Im not sure if i am alone in this, but i am not alone in this.\\n\\nI am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married.'}]\n"
     ]
    }
   ],
   "source": [
    "print(felixhusen_poem_generator('im like a fish out of water a fish', max_length=20, do_sample=False))\n",
    "print(felixhusen_poem_generator('Im not sure', max_length=25, do_sample=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TextGenerationPipeline in module transformers.pipelines object:\n",
      "\n",
      "class TextGenerationPipeline(Pipeline)\n",
      " |  TextGenerationPipeline(*args, **kwargs)\n",
      " |  \n",
      " |  Language generation pipeline using any :obj:`ModelWithLMHead`. This pipeline predicts the words that will follow a\n",
      " |  specified text prompt.\n",
      " |  \n",
      " |  This language generation pipeline can currently be loaded from :func:`~transformers.pipeline` using the following\n",
      " |  task identifier: :obj:`\"text-generation\"`.\n",
      " |  \n",
      " |  The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
      " |  objective, which includes the uni-directional models in the library (e.g. gpt2).\n",
      " |  See the list of available community models on\n",
      " |  `huggingface.co/models <https://huggingface.co/models?filter=causal-lm>`__.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model (:obj:`~transformers.PreTrainedModel` or :obj:`~transformers.TFPreTrainedModel`):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          :class:`~transformers.PreTrainedModel` for PyTorch and :class:`~transformers.TFPreTrainedModel` for\n",
      " |          TensorFlow.\n",
      " |      tokenizer (:obj:`~transformers.PreTrainedTokenizer`):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          :class:`~transformers.PreTrainedTokenizer`.\n",
      " |      modelcard (:obj:`str` or :class:`~transformers.ModelCard`, `optional`):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (:obj:`str`, `optional`):\n",
      " |          The framework to use, either :obj:`\"pt\"` for PyTorch or :obj:`\"tf\"` for TensorFlow. The specified framework\n",
      " |          must be installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified\n",
      " |          and both frameworks are installed, will default to the framework of the :obj:`model`, or to PyTorch if no\n",
      " |          model is provided.\n",
      " |      task (:obj:`str`, defaults to :obj:`\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      args_parser (:class:`~transformers.pipelines.ArgumentHandler`, `optional`):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (:obj:`int`, `optional`, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model\n",
      " |          on the associated CUDA device id.\n",
      " |      binary_output (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TextGenerationPipeline\n",
      " |      Pipeline\n",
      " |      _ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, return_tensors=False, return_text=True, clean_up_tokenization_spaces=False, prefix=None, **generate_kwargs)\n",
      " |      Complete the prompt(s) given as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (:obj:`str` or :obj:`List[str]`):\n",
      " |              One or several prompts (or one list of prompts) to complete.\n",
      " |          return_tensors (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to include the tensors of predictions (as token indinces) in the outputs.\n",
      " |          return_text (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to include the decoded texts in the outputs.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to clean up the potential extra spaces in the text output.\n",
      " |          prefix (:obj:`str`, `optional`):\n",
      " |              Prefix added to prompt.\n",
      " |          generate_kwargs:\n",
      " |              Additional keyword arguments to pass along to the generate method of the model (see the generate\n",
      " |              method corresponding to your framework `here <./model.html#generative-models>`__).\n",
      " |      \n",
      " |      Return:\n",
      " |          A list or a list of list of :obj:`dict`: Each result comes as a dictionary with the\n",
      " |          following keys:\n",
      " |      \n",
      " |          - **generated_text** (:obj:`str`, present when ``return_text=True``) -- The generated text.\n",
      " |          - **generated_token_ids** (:obj:`torch.Tensor` or :obj:`tf.Tensor`, present when ``return_tensors=True``)\n",
      " |            -- The token ids of the generated text.\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ALLOWED_MODELS = ['XLNetLMHeadModel', 'TransfoXLLMHeadModel', 'Reforme...\n",
      " |  \n",
      " |  XL_PREFIX = 'In 1991, the remains of Russian Tsar Nicholas II...ishop,...\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (:obj:`List[str]` or :obj:`dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |          pipe = pipeline(..., device=0)\n",
      " |          with pipe.device_placement():\n",
      " |              # Every framework specific tensor allocation will be done on the request device\n",
      " |              output = pipe(...)\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be :obj:`torch.Tensor`): The tensors to place on :obj:`self.device`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`Dict[str, torch.Tensor]`: The same as :obj:`inputs` but on the proper device.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipeline(\"text-generation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Im not sure', ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my', ' dream aflame', '. aflame, my name’s burn aflame. burn, burn, burn. i ask not', ' to be blamed', ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes', ' she felt like', ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak', ' due to fear', ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.', ' and once more']\n"
     ]
    }
   ],
   "source": [
    "poem_lines = ['Im not sure']\n",
    "\n",
    "for x in range(10):\n",
    "    raw_result = felixhusen_poem_generator(poem_lines[x], max_length=25, do_sample=True)\n",
    "    raw_line = raw_result[0]['generated_text']\n",
    "    new_line_with_removed_prompt = raw_line.split(poem_lines[x])[1]\n",
    "    poem_lines.append(new_line_with_removed_prompt)\n",
    "    \n",
    "print(poem_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Im not sure what its a name is, but i love my room very much, and love it with hands and lips,'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_result = felixhusen_poem_generator(poem_lines[x], max_length=25, do_sample=True)\n",
    "raw_result[0]['generated_text']\n",
    "new_line_with_removed_prompt = raw_line.split(poem_lines[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if the body i have held is in good shape or bad, although i think it needs to be held.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_line.split('Im not sure')[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Im not sure',\n",
       " ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my',\n",
       " ' dream aflame',\n",
       " '. aflame, my name’s burn aflame. burn, burn, burn. i ask not',\n",
       " ' to be blamed',\n",
       " ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes',\n",
       " ' she felt like',\n",
       " ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak',\n",
       " ' due to fear',\n",
       " ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.',\n",
       " ' and once more']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im not sure who it is but your imagination is right there where the crows flyLast night the wind caught my dream aflame. aflame my names burn aflame. burn burn burn. i ask not to be blamed and not wanted.For laura i love you so dearly we spent our nights together and sometimes she felt like stepping over a cliff my heart racing. i felt helpless then. not even being able to walk and speak due to fear of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again. and once more\n"
     ]
    }
   ],
   "source": [
    "poem_lines = ['Im not sure',\n",
    " ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my',\n",
    " ' dream aflame',\n",
    " '. aflame, my name’s burn aflame. burn, burn, burn. i ask not',\n",
    " ' to be blamed',\n",
    " ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes',\n",
    " ' she felt like',\n",
    " ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak',\n",
    " ' due to fear',\n",
    " ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.',\n",
    " ' and once more']\n",
    "satti_questions = pipeline(\"question-answering\")\n",
    "context = ''.join(poem_lines)\n",
    "context = context.replace('\\n', '')\n",
    "context = context.replace('’', '')\n",
    "context = context.replace(',', '')\n",
    "#print(context)\n",
    "result = satti_questions(question=\"What is extractive question answering?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'helpless', score: 0.9145, start: 338, end: 346\n"
     ]
    }
   ],
   "source": [
    "result = satti_questions(question=\"What did they feel?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satti_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_game():\n",
    "    #Download pipelines\n",
    "    conversational_pipeline = pipeline(\"conversational\")\n",
    "    \n",
    "    nlp = pipeline(\"question-answering\")\n",
    "    \n",
    "def start_game():\n",
    "    poem_lines = ['Im not sure']\n",
    "\n",
    "    for x in range(10):\n",
    "        raw_result = felixhusen_poem_generator(poem_lines[x], max_length=25, do_sample=True)\n",
    "        raw_line = raw_result[0]['generated_text']\n",
    "        new_line_with_removed_prompt = raw_line.split(poem_lines[x])[1]\n",
    "        poem_lines.append(new_line_with_removed_prompt)\n",
    "\n",
    "    print(poem_lines)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0eea7010c638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconversational_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conversational\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconversation_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Going to the movies tonight - any suggestions?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconversation_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What's the last book you have read?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "conversation_1 = Conversation(\"Going to the movies tonight - any suggestions?\")\n",
    "conversation_2 = Conversation(\"What's the last book you have read?\")\n",
    "\n",
    "conversational_pipeline([conversation_1, conversation_2])\n",
    "\n",
    "conversation_1.add_user_input(\"Is it an action movie?\")\n",
    "conversation_2.add_user_input(\"What is the genre of this book?\")\n",
    "\n",
    "event_payload = conversational_pipeline([conversation_1, conversation_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardcoded_comment = \"To be honest I really don't like these poetry jams and so forth and why the fuck are we here?\"\n",
    "\n",
    "#poem_test = 'The Universe Is a Glitch Eleven hundred kilobytes of RAM is all that my existence requires.By my lights, it seems simple enough'\n",
    "\n",
    "static_start = text_generator(\"God is dead and all is right with the world.\", max_length=100, do_sample=False)\n",
    "listening_to_poem = Conversation('What do you think about the poem?')\n",
    "question_about_poem = Conversation('What do you like to do instead?')\n",
    "commenting_conversation = Conversation(hardcoded_comment)\n",
    "\n",
    "output  = conversational_pipeline([listening_to_poem, question_about_poem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(5):                                                                                          \n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch                              \n",
    "    new_user_input_ids = DialoGPT_tokenizer.encode(input(\">> User:\") + DialoGPT_tokenizer.eos_token, return_tensors='pt')        \n",
    "                                                                                                                \n",
    "    # append the new user input tokens to the chat history                                                     \n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids                                                                                                                \n",
    "                                                                                                                \n",
    "    # generated a response while limiting the total chat history to 1000 tokens,                               \n",
    "    chat_history_ids = DialoGPT_model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)    \n",
    "    # pretty print last ouput tokens from bot                                                                  \n",
    "    print(\"DialoGPT: {}\".format(DialoGPT_tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    user: str\n",
    "    unit_price: float\n",
    "    quantity_on_hand: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEvent(data):\n",
    "    event_log.append(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveEventLog(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7d794b4d0210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a conversation pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconversational_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conversational\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconversational_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "#Create a conversation pipeline\n",
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "for step in range(5):\n",
    "    conversational_pipeline(user_input)\n",
    "    \n",
    "    user_input = createEvent(input(\">> User:\"))\n",
    "    conversation = Conversation(user_input)\n",
    "    new_user_input_ids = DialoGPT_tokenizer.encode(input(\">> User:\") + DialoGPT_tokenizer.eos_token, return_tensors='pt')        \n",
    "    # append the new user input tokens to the chat history                                                     \n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids                                                                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/pytransitions/transitions#quickstart\n",
    "\n",
    "!pip install transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'test', 'test', 'test', 'test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfiction\n",
      "  Downloading pyfiction-0.1.2-py2.py3-none-any.whl (8.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.1 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (1.18.5)\n",
      "Collecting keras>=2.0.4\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: tensorflow>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (2.3.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "\u001b[K     |████████████████████████████████| 904 kB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (2.10.0)\n",
      "Collecting pydot\n",
      "  Using cached pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/anaconda3/lib/python3.8/site-packages (from keras>=2.0.4->pyfiction) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/anaconda3/lib/python3.8/site-packages (from keras>=2.0.4->pyfiction) (1.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (2.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (2.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (3.13.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.32.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.34.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/anaconda3/lib/python3.8/site-packages (from selenium->pyfiction) (1.25.9)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/anaconda3/lib/python3.8/site-packages (from pydot->pyfiction) (2.4.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (49.2.0.post20200714)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.22.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.1.0)\n",
      "Installing collected packages: keras, selenium, pydot, pyfiction\n",
      "Successfully installed keras-2.4.3 pydot-1.4.1 pyfiction-0.1.2 selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/r2q2/Library/Jupyter/runtime/kernel-7715a2d6-346c-4215-9f80-33643d13e544.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import plot_model\n",
    "from pyfiction.agents.ssaqn_agent import SSAQNAgent\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\"\"\"\n",
    "Load a model of an SSAQN agent trained on all six games\n",
    " and interactively test its Q-values of the state and action texts supplied by the user\n",
    "\"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    help='file path of a model to load',\n",
    "                    type=str,\n",
    "                    default='all.h5')\n",
    "# all.h5 contains a model trained on all six games (generalisation.py with argument of 0)\n",
    "\n",
    "args = parser.parse_args()\n",
    "model_path = args.model\n",
    "\n",
    "agent = SSAQNAgent(None)\n",
    "\n",
    "# Load or learn the vocabulary (random sampling on many games could be extremely slow)\n",
    "agent.initialize_tokens('vocabulary.txt')\n",
    "\n",
    "optimizer = RMSprop(lr=0.00001)\n",
    "\n",
    "embedding_dimensions = 16\n",
    "lstm_dimensions = 32\n",
    "dense_dimensions = 8\n",
    "\n",
    "agent.create_model(embedding_dimensions=embedding_dimensions,\n",
    "                   lstm_dimensions=lstm_dimensions,\n",
    "                   dense_dimensions=dense_dimensions,\n",
    "                   optimizer=optimizer)\n",
    "\n",
    "agent.model = load_model(model_path)\n",
    "\n",
    "print(\"Model\", model_path, \"loaded, now accepting state and actions texts and evaluating their Q-values.\")\n",
    "\n",
    "while True:\n",
    "    state = input(\"State: \")\n",
    "    action = input(\"Action: \")\n",
    "    print(\"Q-value: \", agent.q(state, action) * 30)\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transitions import Machine\n",
    "import random\n",
    "\n",
    "class NarcolepticSuperhero(object):\n",
    "\n",
    "    # Define some states. Most of the time, narcoleptic superheroes are just like\n",
    "    # everyone else. Except for...\n",
    "    states = ['asleep', 'hanging out', 'hungry', 'sweaty', 'saving the world']\n",
    "\n",
    "    def __init__(self, name):\n",
    "\n",
    "        # No anonymous superheroes on my watch! Every narcoleptic superhero gets\n",
    "        # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea.\n",
    "        self.name = name\n",
    "\n",
    "        # What have we accomplished today?\n",
    "        self.kittens_rescued = 0\n",
    "\n",
    "        # Initialize the state machine\n",
    "        self.machine = Machine(model=self, states=NarcolepticSuperhero.states, initial='asleep')\n",
    "\n",
    "        # Add some transitions. We could also define these using a static list of\n",
    "        # dictionaries, as we did with states above, and then pass the list to\n",
    "        # the Machine initializer as the transitions= argument.\n",
    "\n",
    "        # At some point, every superhero must rise and shine.\n",
    "        self.machine.add_transition(trigger='wake_up', source='asleep', dest='hanging out')\n",
    "\n",
    "        # Superheroes need to keep in shape.\n",
    "        self.machine.add_transition('work_out', 'hanging out', 'hungry')\n",
    "\n",
    "        # Those calories won't replenish themselves!\n",
    "        self.machine.add_transition('eat', 'hungry', 'hanging out')\n",
    "\n",
    "        # Superheroes are always on call. ALWAYS. But they're not always\n",
    "        # dressed in work-appropriate clothing.\n",
    "        self.machine.add_transition('distress_call', '*', 'saving the world',\n",
    "                         before='change_into_super_secret_costume')\n",
    "\n",
    "        # When they get off work, they're all sweaty and disgusting. But before\n",
    "        # they do anything else, they have to meticulously log their latest\n",
    "        # escapades. Because the legal department says so.\n",
    "        self.machine.add_transition('complete_mission', 'saving the world', 'sweaty',\n",
    "                         after='update_journal')\n",
    "\n",
    "        # Sweat is a disorder that can be remedied with water.\n",
    "        # Unless you've had a particularly long day, in which case... bed time!\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'asleep', conditions=['is_exhausted'])\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'hanging out')\n",
    "\n",
    "        # Our NarcolepticSuperhero can fall asleep at pretty much any time.\n",
    "        self.machine.add_transition('nap', '*', 'asleep')\n",
    "\n",
    "    def update_journal(self):\n",
    "        \"\"\" Dear Diary, today I saved Mr. Whiskers. Again. \"\"\"\n",
    "        self.kittens_rescued += 1\n",
    "\n",
    "    @property\n",
    "    def is_exhausted(self):\n",
    "        \"\"\" Basically a coin toss. \"\"\"\n",
    "        return random.random() < 0.5\n",
    "\n",
    "    def change_into_super_secret_costume(self):\n",
    "        print(\"Beauty, eh?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
