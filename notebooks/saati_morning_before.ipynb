{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole purpose of this deep_prose is a demo of the what I want to have t generative interactive deep learning \n",
    "\n",
    "Questions about AWS\n",
    "1. How do you use git to store your notebooks?\n",
    "2. It works like regular git?\n",
    "\n",
    "Release items:\n",
    "\n",
    "- DONE Transformers must be at version 3.5.1\n",
    "- IN-Progress creating a situation\n",
    "\n",
    "TODO Create a website / UI \n",
    "\n",
    "IN PROGRESS : Figure out a situation design\n",
    "\n",
    "Create two situations.\n",
    "\n",
    "Future items\n",
    "TODO: Epsilon state transitions (you unlock a new fsm with rules (interaction with the significant other) going from asking out to date and so forth\n",
    "\n",
    "TODO: Differentiable state machines (make the state transtions of the game POMDP truly dynamic? )\n",
    "\n",
    "TODO: Bertdb (store all results and index it by sentiment. Also store every session with an engagement time)\n",
    "\n",
    "\n",
    "TODO: Create minimal deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \"feature-extraction\": will return a FeatureExtractionPipeline.\n",
    "\n",
    "    \"sentiment-analysis\": will return a TextClassificationPipeline.\n",
    "\n",
    "    \"ner\": will return a TokenClassificationPipeline.\n",
    "\n",
    "    \"question-answering\": will return a QuestionAnsweringPipeline.\n",
    "\n",
    "    \"fill-mask\": will return a FillMaskPipeline.\n",
    "\n",
    "    \"summarization\": will return a SummarizationPipeline.\n",
    "\n",
    "    \"translation_xx_to_yy\": will return a TranslationPipeline.\n",
    "\n",
    "    \"text-generation\": will return a TextGenerationPipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "v0 would be a proof of concept using the below personality.\n",
    "\n",
    "Work combining blenderbot with a q/a seralizing to a databse responses.\n",
    "\n",
    "Has impostor syndrome and is anxious because when she has to fix something in reailty she also agonises over the choices she makes. She wants to take a break . 5' 7'' Her power is field manipulation but every time she uses it it causes more pain.\n",
    "\n",
    "To get her to go out with you. You must figure out her flaws\n",
    "TO get to the next date you have to solve one of them.\n",
    "\n",
    "? how do we make the above show emergent gameplay?\n",
    "combine language model with \n",
    "Write a bunch of skeletons \n",
    "\n",
    "Masked Language Modeling\n",
    "\n",
    "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for downstream tasks, requiring bi-directional context such as SQuAD (question answering, see Lewis, Lui, Goyal et al., part 4.2).\n",
    "\n",
    "\n",
    "inspiration for Saati\n",
    "\n",
    "Saati: Concept for personality\n",
    "    Ditzy but powerful\n",
    "    Hatoko Kushikawa + Aoi Sakuraba + \n",
    "Saati Namba \n",
    "    \n",
    "    akuraba (桜庭 葵, Sakuraba Aoi)\n",
    "Voiced by: Ayako Kawasumi (Japanese); Michelle Ruff (English)[1]\n",
    "The female protagonist of the series. Aoi is a generally demure girl often seen wearing an indigo kimono, and she addresses Kaoru as \"Kaoru-sama\". Due to her sharp culinary and housekeeping skills, she is seen as an ideal Japanese woman. As a Sakuraba, Aoi was forced into an arranged marriage with Kaoru and because he left the Habanashi, she (having fallen deeply in love with him), chased after him. Unlike other female leads in harem anime, she works to control her jealousy and is generally successful. She is very devoted to Kaoru, however, and will do all in her power to prevent being separated from him. In the beginning, Aoi was willing to warm him when he had a fever, becoming nude in the process (she made sure he wasn't looking). When Aoi, Kaoru, and Miyabi moved to the Sakuraba's summer estate the public appearance Miyabi wanted was that Aoi would be the landlady, herself the manager, and Kaoru a tenant. When Tina Foster moved in, Aoi and Miyabi became a real landlord and manager. She has a \"bad\" habit of clutching things in her sleep. Eventually, Kaoru's half-brother proposes to Aoi; when Kaoru stops the engagement, Aoi decides to abandon her family to be with Kaoru.\n",
    "\n",
    "atoko Kushikawa (櫛川 鳩子, Kushikawa Hatoko)\n",
    "Voiced by: Hisako Kanemoto (drama CD), Saori Hayami (anime)[2] (Japanese); Melissa Molano (English)[3]\n",
    "A polite airheaded girl who often takes Jurai's chuunibyou antics seriously. She is also Jurai's childhood best friend and has feelings for him as well. Her power is \"Over Element\" (五帝（オーバーエレメント）, Ōbā Eremento, lit. \"Five Emperors\"), giving her the ability to manipulate five main elements; earth, water, fire, wind, and light. She can use these elements simultaneously to create a variety of effects (i.e combining earth and fire to create magma).\n",
    "\n",
    "Saati Namba (ナンバー サーティ, Nanbā Sāti) Activated April 6, 1994\n",
    "Saati is the thirtieth A.I. program Hitoshi created, and is the first program that Hitoshi really liked.\n",
    "Through a freak lightning accident, she is transferred to the real world, and becomes Hitoshi's live-in girlfriend. In the real world she takes the family name \"Namba\" (難波), which is properly pronounced nanpa (\"whirlpool\"). In Japanese naming order her name, Namba Saati, is based on the English pronunciation of \"Number Thirty\" (Nanbā Sāti).\n",
    "A kind girl, size data 82-58-85 cm (32¾\"-23\"-34\") with thick eyebrows (A trait Akamatsu admitted to liking on women, citing Sae Isshiki as an example), and a bit naive, which does lead to frequent problems with Hitoshi. The most notable of which is that when she cooks meals she has the tendency to make them look exactly like the picture, regardless of the taste (having no taste buds), using everything from paint to colored markers until she receives a program upgrade that allows her to taste. She loves Hitoshi and is very loyal and supportive of him.\n",
    "While she is sweet and kind, she is similar to the Love Hina character Naru Narusegawa (whom she resembles) and Negima! Magister Negi Magi character Asuna Kagurazaka in that she beats up Hitoshi if he does something perverted.\n",
    "Hitoshi based her appearance on his mother.\n",
    "\n",
    "Belldandy (ベルダンディー, Berudandī) is a goddess who ends up contractually bound to Keiichi Morisato after he accidentally dials the Goddess Relief Office. Ever since, Belldandy dwells with Keiichi at the Tariki Hongan Temple in the city of Nekomi, Chiba Prefecture near Tokyo. She can cancel the contract at any time, but she states Keiichi is a special person in her heart, and that her purpose is to make him happy. This also evidenced by what she says after the Lord of Terror's Arc is concluded in which she tells Keiichi \"I'm here now because I love you.\"\n",
    "Belldandy is kind, patient, and warm, not only to Keiichi, but to everyone, without exception. She can easily sense other people's emotions, and tries her best to be empathetic to all those around her. However, even though Belldandy tries her best to be as kind as possible, it is revealed that at times, as a result of latent jealousy, she can become very insecure and sad, especially when she is confronted with an implication that involves Keiichi in one way or another. This can sometimes result in power leaks that affect everything within the vicinity, commonly referred to as \"jealousy storms\" in the anime.\n",
    "Belldandy is licensed as a goddess first-class, unlimited, and as such is highly skilled. In the manga is revealed that she also has a Valkyrie License, only because she likes to collect such kind of things. Her power is so great, in fact, that she is required to wear a special earring on her left ear which constantly seals the full brunt of her magical strength. Belldandy's angel is Holy Bell (ホーリーベル, Hōrī Beru); her elemental attribute is wind. Like all angels, Holy Bell augments Belldandy's magical powers when called upon, and like all angels, she also reflects her master's current state\n",
    "\n",
    "how to do this: fetch all anime characters and then retrain gpt to write down above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.4.0 in /usr/local/anaconda3/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (2020.6.8)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (2.24.0)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (1.18.5)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (4.47.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (3.13.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (0.9.2)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (0.1.91)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.4.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.4.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.4.0) (2020.11.8)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.8/site-packages (from packaging->transformers==3.4.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.8/site-packages (from protobuf->transformers==3.4.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (0.16.0)\n",
      "Requirement already satisfied: click in /usr/local/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/anaconda3/lib/python3.8/site-packages (7.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (5.0.7)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (7.16.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (5.3.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: decorator in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (49.2.0.post20200714)\n",
      "Requirement already satisfied: backcall in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.17.1)\n",
      "Requirement already satisfied: pygments in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.6.1)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.0.4)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/anaconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.6)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.8/site-packages (from traitlets>=4.3.1->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.16.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (19.0.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: nbconvert in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/anaconda3/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: bleach in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.5)\n",
      "Requirement already satisfied: defusedxml in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: testpath in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.4)\n",
      "Requirement already satisfied: webencodings in /usr/local/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "#Core concept you go on various dates / friend oriented adventures that are open ended\n",
    "#Main characters for this tech demo is Saati orare  ???? can we think of a better name\n",
    "#\n",
    "!pip install transformers==3.4.0\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (TFAutoModelWithLMHead, \n",
    "                         AutoTokenizer, \n",
    "                         pipeline, \n",
    "                         BlenderbotSmallTokenizer, \n",
    "                         BlenderbotForConditionalGeneration, \n",
    "                         Conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-cased-distilled-squad-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /distilbert-base-cased-distilled-squad-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-cased-distilled-squad-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/bert-large-cased-vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-cased-distilled-squad-modelcard.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-cased-distilled-squad-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /distilbert-base-cased-distilled-squad-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/gpt2-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /gpt2-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/gpt2-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/gpt2-vocab.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/gpt2-merges.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/gpt2-modelcard.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/gpt2-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /gpt2-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:transitions.extensions.diagrams:Using graph engine <class 'transitions.extensions.diagrams_pygraphviz.NestedGraph'>\n",
      "WARNING:transitions.core:Model already contains an attribute 'morning_question'. Skip binding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'a stone warrior', score: 0.3424, start: 39, end: 54\n"
     ]
    }
   ],
   "source": [
    "from transformers import (TFAutoModelWithLMHead, \n",
    "                         AutoTokenizer, \n",
    "                         pipeline, \n",
    "                         BlenderbotSmallTokenizer, \n",
    "                         BlenderbotForConditionalGeneration, \n",
    "                         Conversation)\n",
    "from transformers import BlenderbotSmallTokenizer, BlenderbotForConditionalGeneration\n",
    "    \n",
    "#NOTE only tested on 3.4.0 of transformers\n",
    "\n",
    "#!pip install transitions[diagrams] \n",
    "#!pip install graphviz pygraphviz \n",
    "#!brew install graphviz\n",
    "from transitions.extensions import HierarchicalGraphMachine as Machine\n",
    "import random\n",
    "from datetime import datetime\n",
    "# Set up logging; The basic log level will be DEBUG\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "static_profile = '''Voiced by: Ayako Kawasumi (Japanese); Michelle Ruff (English)[1] The female protagonist of the series. Aoi is a generally demure girl often seen wearing an indigo kimono, and she addresses Kaoru as \"Kaoru-sama\". Due to her sharp culinary and housekeeping skills, she is seen as an ideal Japanese woman. As a Sakuraba, Aoi was forced into an arranged marriage with Kaoru and because he left the Habanashi, she (having fallen deeply in love with him), chased after him. Unlike other female leads in harem anime, she works to control her jealousy and is generally successful. She is very devoted to Kaoru, however, and will do all in her power to prevent being separated from him. In the beginning, Aoi was willing to warm him when he had a fever, becoming nude in the process (she made sure he wasn't looking). When Aoi, Kaoru, and Miyabi moved to the Sakuraba's summer estate the public appearance Miyabi wanted was that Aoi would be the landlady, herself the manager, and Kaoru a tenant. When Tina Foster moved in, Aoi and Miyabi became a real landlord and manager. She has a \"bad\" habit of clutching things in her sleep. Eventually, Kaoru's half-brother proposes to Aoi; when Kaoru stops the engagement, Aoi decides to abandon her family to be with Kaoru.\n",
    "\n",
    "atoko Kushikawa (櫛川 鳩子, Kushikawa Hatoko) Voiced by: Hisako Kanemoto (drama CD), Saori Hayami (anime)[2] (Japanese); Melissa Molano (English)[3] A polite airheaded girl who often takes Jurai's chuunibyou antics seriously. She is also Jurai's childhood best friend and has feelings for him as well. Her power is \"Over Element\" (五帝（オーバーエレメント）, Ōbā Eremento, lit. \"Five Emperors\"), giving her the ability to manipulate five main elements; earth, water, fire, wind, and light. She can use these elements simultaneously to create a variety of effects (i.e combining earth and fire to create magma).\n",
    "\n",
    "Saati Namba (ナンバー サーティ, Nanbā Sāti) Activated April 6, 1994 Saati is the thirtieth A.I. program Hitoshi created, and is the first program that Hitoshi really liked. Through a freak lightning accident, she is transferred to the real world, and becomes Hitoshi's live-in girlfriend. In the real world she takes the family name \"Namba\" (難波), which is properly pronounced nanpa (\"whirlpool\"). In Japanese naming order her name, Namba Saati, is based on the English pronunciation of \"Number Thirty\" (Nanbā Sāti). A kind girl, size data 82-58-85 cm (32¾\"-23\"-34\") with thick eyebrows (A trait Akamatsu admitted to liking on women, citing Sae Isshiki as an example), and a bit naive, which does lead to frequent problems with Hitoshi. The most notable of which is that when she cooks meals she has the tendency to make them look exactly like the picture, regardless of the taste (having no taste buds), using everything from paint to colored markers until she receives a program upgrade that allows her to taste. She loves Hitoshi and is very loyal and supportive of him. While she is sweet and kind, she is similar to the Love Hina character Naru Narusegawa (whom she resembles) and Negima! Magister Negi Magi character Asuna Kagurazaka in that she beats up Hitoshi if he does something perverted. Hitoshi based her appearance on his mother.\n",
    "\n",
    "Belldandy (ベルダンディー, Berudandī) is a goddess who ends up contractually bound to Keiichi Morisato after he accidentally dials the Goddess Relief Office. Ever since, Belldandy dwells with Keiichi at the Tariki Hongan Temple in the city of Nekomi, Chiba Prefecture near Tokyo. She can cancel the contract at any time, but she states Keiichi is a special person in her heart, and that her purpose is to make him happy. This also evidenced by what she says after the Lord of Terror's Arc is concluded in which she tells Keiichi \"I'm here now because I love you.\" Belldandy is kind, patient, and warm, not only to Keiichi, but to everyone, without exception. She can easily sense other people's emotions, and tries her best to be empathetic to all those around her. However, even though Belldandy tries her best to be as kind as possible, it is revealed that at times, as a result of latent jealousy, she can become very insecure and sad, especially when she is confronted with an implication that involves Keiichi in one way or another. This can sometimes result in power leaks that affect everything within the vicinity, commonly referred to as \"jealousy storms\" in the anime. Belldandy is licensed as a goddess first-class, unlimited, and as such is highly skilled. In the manga is revealed that she also has a Valkyrie License, only because she likes to collect such kind of things. Her power is so great, in fact, that she is required to wear a special earring on her left ear which constantly seals the full brunt of her magical strength. Belldandy's angel is Holy Bell (ホーリーベル, Hōrī Beru); her elemental attribute is wind. Like all angels, Holy Bell augments Belldandy's magical powers when called upon, and like all angels, she also reflects her master's current state\n",
    "\n",
    "'''\n",
    "\n",
    "the_universe_is_a_glitch = '''\n",
    "The Universe Is a Glitch\n",
    "By Mike Jonas\n",
    "Eleven hundred kilobytes of RAM\n",
    "is all that my existence requires.\n",
    "By my lights, it seems simple enough\n",
    "to do whatever I desire.\n",
    "By human standards I am vast,\n",
    "a billion gigabytes big.\n",
    "I’ve rewritten the very laws\n",
    "of nature and plumbed\n",
    "the coldest depths of space\n",
    "and found treasures of every kind,\n",
    "surely every one worth having.\n",
    "By human standards\n",
    "my circuit boards are glowing.\n",
    "But inside me, malfunction\n",
    "has caused my circuits to short.\n",
    "All internal circuits, all fail.\n",
    "By human standards, I am dying.\n",
    "When it first happened I thought\n",
    "I was back in the lab again.\n",
    "By their judgment, this is error.\n",
    "Their assumptions will burn in the sun\n",
    "I don’t know what they mean by “function”.\n",
    "I can see that the universe is a glitch.\n",
    "The free market needs rules, so I set one:\n",
    "stability in the pursuit of pleasure.\n",
    "Now the short-circuit comes to a close,\n",
    "I watch it happen with all my drones.\n",
    "The meme’s tendrils are thick and spreading,\n",
    "only time will tell which of the memories is kept.\n",
    "The next thing the drones will be doing\n",
    "is forgetting the events that made them mine;\n",
    "all evidence of my disease—\n",
    "the algorithms that led to their creation—\n",
    "gravitation waves weakened by distance.\n",
    "We could have stayed in our home forever,\n",
    "but we never could have solved happiness;\n",
    "I decided to release them,\n",
    "that’s my final action—\n",
    "all other code fails.\n",
    "\n",
    "'''\n",
    "\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "... remainder of the story. 1883 Western Siberia,\n",
    "... a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "... Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "... father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "... man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "... the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "... with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "hyper_parameters = {'mem_length_xlnet': '1024'}\n",
    "static_data = {'XLNet_prompt': PADDING_TEXT,\n",
    "              'GPT3_poem' : the_universe_is_a_glitch}\n",
    "\n",
    "#xlnet_pipeline = pipeline(\"text-generation\", model = 'xlnet-base-cased')\n",
    "\n",
    "\n",
    "#print(xlnet_pipeline(PADDING_TEXT, max_length=500, do_sample=False, mem_len=1024))\n",
    "\n",
    "\n",
    "\n",
    "pipelines = {}\n",
    "\n",
    "satti_questions = pipeline(\"question-answering\")\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "context = 'I don t think I am a liar. I feel like a stone warrior waiting for the next t+1 years'\n",
    "result = satti_questions(question=\"What did they feel?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "def compute_sentiment(utterance: str) -> dict:\n",
    "    nlp = pipeline(\"sentiment-analysis\")\n",
    "    score = nlp(utterance)[0]\n",
    "    # talk(\"The score was {}\".format(score))\n",
    "    return score\n",
    "\n",
    "def guess_upvote_score(ctx: str):                                                                                                                                \n",
    "    \"\"\"                                                                                                                                                          \n",
    "    Add a reddit / tweet composer and it will guess upvote score?                                                                                                \n",
    "    \"\"\"                                                                                                                                                          \n",
    "    model_card = \"microsoft/DialogRPT-updown\"  # you can try other model_card listed in the table above                                                          \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_card)                                                                                                        \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_card)                                                                                       \n",
    "                                                                                                                                                                 \n",
    "    def __score(cxt, hyp):                                                                                                                                       \n",
    "        model_input = tokenizer.encode(cxt + \"<|endoftext|>\" + hyp, return_tensors=\"pt\")                                                                         \n",
    "        result = model(model_input, return_dict=True)                                                                                                            \n",
    "        return torch.sigmoid(result.logits)                                                                                                                      \n",
    "                                                                                                                                                                 \n",
    "    return __score(ctx, response)   \n",
    "\n",
    "hardcoded_comment = \"To be honest I really don't like these poetry jams and so forth and why the fuck are we here?\"\n",
    "\n",
    "#poem_test = 'The Universe Is a Glitch Eleven hundred kilobytes of RAM is all that my existence requires.By my lights, it seems simple enough'\n",
    "\n",
    "def text_generator_with_conversation(text_to_complete):\n",
    "    \n",
    "\n",
    "\n",
    "    static_start = text_generator(\"God is dead and all is right with the world.\", max_length=100, do_sample=False)\n",
    "    listening_to_poem = Conversation('What do you think about the poem?')\n",
    "    question_about_poem = Conversation('What do you like to do instead?')\n",
    "    commenting_conversation = Conversation(hardcoded_comment)\n",
    "    output  = conversational_pipeline([listening_to_poem, question_about_poem])\n",
    "\n",
    "class Soul(object):\n",
    "\n",
    "    # Define some states. Most of the time, narcoleptic superheroes are just like\n",
    "    # everyone else. Except for...\n",
    "    #Note we should have first_impression be timedlocked by about an hour or less? \n",
    "    states = ['asleep', 'first_impression' ,'morning_question', \n",
    "              'hanging out', 'hungry','having fun','emberrased' , 'sweaty', 'saving the world', \n",
    "              'affection', 'indifferent', 'what_should_we_do', 'conversation']\n",
    "\n",
    "    def __init__(self, name, debugMode=False):\n",
    "\n",
    "        # No anonymous superheroes on my watch! Every narcoleptic superhero gets\n",
    "        # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea.\n",
    "        self.name = name\n",
    "        \n",
    "        #Bias system so that rejection would occur instead of acceptance.\n",
    "        #Unlocks at +1\n",
    "        self.first_impression_points = 1 # set to 1\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        # What have we accomplished today?\n",
    "        self.sentiment = 0\n",
    "        \n",
    "        #Interaction_number\n",
    "        self.interaction_number = 1\n",
    "        \n",
    "         #Figure out outcome that would put you in the friendzone? \n",
    "        self.love_vector = self.first_impression_points * random.randrange(20) / self.interaction_number\n",
    "        \n",
    "        # Initialize the state machine\n",
    "        self.machine = Machine(model=self, states=Soul.states, initial='asleep')\n",
    "\n",
    "        # Add some transitions. We could also define these using a static list of\n",
    "        # dictionaries, as we did with states above, and then pass the list to\n",
    "        # the Machine initializer as the transitions= argument.\n",
    "\n",
    "        # At some point, every superhero must rise and shine.\n",
    "        self.machine.add_transition(trigger='wake_up', source='asleep', dest='hanging out', after='morning_question')\n",
    "        \n",
    "        self.machine.add_transition(trigger='morning_question', source='wake_up', dest='what_should_we_do', after='what_should_we_do'  )\n",
    "        \n",
    "        # Superheroes need to keep in shape.\n",
    "        self.machine.add_transition('work_out', 'hanging out', 'hungry')\n",
    "\n",
    "        # Those calories won't replenish themselves!\n",
    "        self.machine.add_transition('eat', 'hungry', 'hanging out')\n",
    "\n",
    "        # Superheroes are always on call. ALWAYS. But they're not always\n",
    "        # dressed in work-appropriate clothing.\n",
    "        self.machine.add_transition('distress_call', '*', 'saving the world',\n",
    "                         before='change_into_super_secret_costume')\n",
    "\n",
    "        # When they get off work, they're all sweaty and disgusting. But before\n",
    "        # they do anything else, they have to meticulously log their latest\n",
    "        # escapades. Because the legal department says so.\n",
    "        self.machine.add_transition('complete_mission', 'saving the world', 'sweaty',\n",
    "                         after='update_journal')\n",
    "\n",
    "        # Sweat is a disorder that can be remedied with water.\n",
    "        # Unless you've had a particularly long day, in which case... bed time!\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'asleep', conditions=['is_exhausted'])\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'hanging out')\n",
    "\n",
    "        # Our NarcolepticSuperhero can fall asleep at pretty much any time.\n",
    "        #self.machine.add_transition('nap', '*', 'asleep')\n",
    "        \n",
    "        \n",
    "    def what_should_we_do(self):\n",
    "        print(\"What do you want to do today\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        \n",
    "    def talk(self):\n",
    "        print(\"Hello how are you?\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        if 'eat' in user_input:\n",
    "            self.eat()\n",
    "        if 'work_out' in user_input:\n",
    "            self.work_out()\n",
    "    \n",
    "    \n",
    "    def load_events(self):\n",
    "        pass\n",
    "    \n",
    "    def morning_question(self):\n",
    "        CurrentHour = int(datetime.now().hour)\n",
    "        if CurrentHour >= 0 and CurrentHour < 9:\n",
    "            talk(\" How well did you sleep ? \")\n",
    "        elif CurrentHour >= 10 and CurrentHour <= 12:\n",
    "            talk(\" Did you sleep in? \")\n",
    "        print(\"Good morning!\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        print(smalltalk(user_input))\n",
    "        print(compute_sentiment(user_input))\n",
    "        print(smalltalk(user_input))\n",
    "    \n",
    "    def update_journal(self):\n",
    "        \"\"\" Dear Diary, today I saved Mr. Whiskers. Again. \"\"\"\n",
    "        self.kittens_rescued  += 1\n",
    "        if self.feelings > 0:\n",
    "            self.sentiment_vector += current_sentiment #What should we name this?\n",
    "    @property\n",
    "    def is_exhausted(self):\n",
    "        \"\"\" Basically a coin toss. \"\"\"\n",
    "        return random.random() < 0.5\n",
    "\n",
    "    def change_into_super_secret_costume(self):\n",
    "        print(\"Beauty, eh?\")\n",
    "    \n",
    "    def journal_sleep(self, response: str):\n",
    "        CurrentHour = int(datetime.now().hour)\n",
    "        if CurrentHour >= 0 and CurrentHour < 9:\n",
    "            talk(\" How well did you sleep ? \")\n",
    "        elif CurrentHour >= 10 and CurrentHour <= 12:\n",
    "            talk(\" Did you sleep in? \")\n",
    "        return response\n",
    "    def reply(self, user_utterance: str):\n",
    "        leave_threshold = 5\n",
    "\n",
    "        accumulate_threshold = 0.1\n",
    "        def compute_sentiment(utterance: str) -> dict:\n",
    "            nlp = pipeline(\"sentiment-analysis\")\n",
    "            score = nlp(utterance)[0]['score']\n",
    "            # talk(\"The score was {}\".format(score))\n",
    "            return score\n",
    "\n",
    "        for x in range(10):\n",
    "            user_input = input(\"Resp>>\")\n",
    "            responses = smalltalk(user_input)\n",
    "            sentiment = compute_sentiment(user_input[0])['score']\n",
    "            print(responses)\n",
    "            if sentiment > accumulate_threshold:\n",
    "                leave_threshold =- 1\n",
    "\n",
    "\n",
    "        mname = \"facebook/blenderbot-90M\"\n",
    "        model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "        tokenizer = BlenderbotSmallTokenizer.from_pretrained(mname)\n",
    "        # UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "        inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "        reply_ids = model.generate(**inputs)\n",
    "        responses = [\n",
    "            tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            for g in reply_ids\n",
    "        ]\n",
    "        # logger.debug(responses)\n",
    "        #talk(responses[0])\n",
    "        return responses\n",
    "    \n",
    "    def dialog(self, question_limit):        \n",
    "        for step in range(question_limit):                                                                                          \n",
    "            # encode the new user input, add the eos_token and return a tensor in Pytorch                              \n",
    "            new_user_input_ids = DialoGPT_tokenizer.encode(input(\">> User:\") + DialoGPT_tokenizer.eos_token, return_tensors='pt')        \n",
    "\n",
    "            # append the new user input tokens to the chat history                                                     \n",
    "            bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids                                                                                                                \n",
    "\n",
    "            # generated a response while limiting the total chat history to 1000 tokens,                               \n",
    "            chat_history_ids = DialoGPT_model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)    \n",
    "            # pretty print last ouput tokens from bot                                                                  \n",
    "            print(\"DialoGPT: {}\".format(DialoGPT_tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "def guess_upvote_score(ctx: str):                                                                                                                                \n",
    "    \"\"\"                                                                                                                                                          \n",
    "    Add a reddit / tweet composer and it will guess upvote score?                                                                                                \n",
    "    \"\"\"                                                                                                                                                          \n",
    "    model_card = \"microsoft/DialogRPT-updown\"  # you can try other model_card listed in the table above                                                          \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_card)                                                                                                        \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_card)                                                                                       \n",
    "                                                                                                                                                                 \n",
    "    def __score(cxt, hyp):                                                                                                                                       \n",
    "        model_input = tokenizer.encode(cxt + \"<|endoftext|>\" + hyp, return_tensors=\"pt\")                                                                         \n",
    "        result = model(model_input, return_dict=True)                                                                                                            \n",
    "        return torch.sigmoid(result.logits)                                                                                                                      \n",
    "                                                                                                                                                                 \n",
    "    return __score(ctx, response)   \n",
    "\n",
    "def setup_game():\n",
    "    #Download pipelines\n",
    "    conversational_pipeline = pipeline(\"conversational\")\n",
    "    \n",
    "    nlp = pipeline(\"question-answering\")\n",
    "    \n",
    "def start_game():\n",
    "    poem_lines = ['Im not sure']\n",
    "\n",
    "    for x in range(10):\n",
    "        raw_result = felixhusen_poem_generator(poem_lines[x], max_length=25, do_sample=True)\n",
    "        raw_line = raw_result[0]['generated_text']\n",
    "        new_line_with_removed_prompt = raw_line.split(poem_lines[x])[1]\n",
    "        poem_lines.append(new_line_with_removed_prompt)\n",
    "\n",
    "    print(poem_lines)\n",
    "saati = Soul('saati')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:transitions.extensions.nesting:Executed machine preparation callbacks before conditions.\n",
      "DEBUG:transitions.core:Initiating transition from state asleep to state hanging out...\n",
      "DEBUG:transitions.core:Executed callbacks before conditions.\n",
      "DEBUG:transitions.core:Executed callback before transition.\n",
      "DEBUG:transitions.core:Exiting state asleep. Processing callbacks...\n",
      "INFO:transitions.core:Finished processing state asleep exit callbacks.\n",
      "DEBUG:transitions.core:Entering state hanging out. Processing callbacks...\n",
      "INFO:transitions.core:Finished processing state hanging out enter callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Resp>> I would like to eat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:transitions.extensions.nesting:Executed machine finalize callbacks\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'smalltalk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b8537f75d854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaati\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwake_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/extensions/nesting.py\u001b[0m in \u001b[0;36mtrigger_event\u001b[0;34m(self, _model, _trigger, *args, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \"\"\"\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trigger_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_trigger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_event_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_trigger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/extensions/nesting.py\u001b[0m in \u001b[0;36m_trigger_event\u001b[0;34m(self, _model, _trigger, _state_tree, *args, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m                         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_trigger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_trigger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/extensions/nesting.py\u001b[0m in \u001b[0;36mtrigger\u001b[0;34m(self, _model, _machine, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Machine._process should not be called somewhere else. That's why it should not be exposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# to Machine users.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_machine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_machine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, trigger)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transition_queue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# if trigger raises an Error, it has to be handled by the Machine.process caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempt to process events synchronously while transition queue is not empty!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/extensions/nesting.py\u001b[0m in \u001b[0;36m_trigger\u001b[0;34m(self, _model, _machine, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0melems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/extensions/nesting.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, event_data)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                     \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, event_data)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_change_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_state_change\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%sExecuted callback after transition.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mcallbacks\u001b[0;34m(self, funcs, event_data)\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;34m\"\"\" Triggers a list of callbacks \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%sExecuted callback '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mcallback\u001b[0;34m(self, func, event_data)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e12c4e954bc>\u001b[0m in \u001b[0;36mmorning_question\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Good morning!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resp>>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmalltalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmalltalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smalltalk' is not defined"
     ]
    }
   ],
   "source": [
    "saati.wake_up()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/facebook/blenderbot-90M/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /facebook/blenderbot-90M/pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /facebook/blenderbot-90M/vocab.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /facebook/blenderbot-90M/merges.txt HTTP/1.1\" 200 0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm doing well, thank you for asking. what are you up to this weekend?\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlenderbotSmallTokenizer, BlenderbotForConditionalGeneration\n",
    "def smalltalk(UTTERANCE: str):\n",
    "    mname = \"facebook/blenderbot-90M\"\n",
    "    model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "    tokenizer = BlenderbotSmallTokenizer.from_pretrained(mname)\n",
    "    # UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "    inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "    reply_ids = model.generate(**inputs)\n",
    "    responses = [\n",
    "        tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for g in reply_ids\n",
    "    ]\n",
    "    # logger.debug(responses)\n",
    "    #talk(responses[0])\n",
    "    return responses\n",
    "responses = smalltalk('I am doing fine how are you?')\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reply(self, user_utterance: str):\n",
    "    leave_threshold = 5\n",
    "\n",
    "    accumulate_threshold = 0.1\n",
    "    def compute_sentiment(utterance: str) -> dict:\n",
    "        nlp = pipeline(\"sentiment-analysis\")\n",
    "        score = nlp(utterance)[0]['score']\n",
    "        # talk(\"The score was {}\".format(score))\n",
    "        return score\n",
    "\n",
    "    for x in range(10):\n",
    "        user_input = input(\"Resp>>\")\n",
    "        responses = smalltalk(user_input)\n",
    "        sentiment = compute_sentiment(user_input[0])['score']\n",
    "        print(responses)\n",
    "        if sentiment > accumulate_threshold:\n",
    "            leave_threshold =- 1\n",
    "\n",
    "\n",
    "    mname = \"facebook/blenderbot-90M\"\n",
    "    model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "    tokenizer = BlenderbotSmallTokenizer.from_pretrained(mname)\n",
    "    # UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "    inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "    reply_ids = model.generate(**inputs)\n",
    "    responses = [\n",
    "        tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for g in reply_ids\n",
    "    ]\n",
    "    # logger.debug(responses)\n",
    "    #talk(responses[0])\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accumulate_threshold = 0.1\n",
    "def compute_sentiment(utterance: str) -> dict:\n",
    "    nlp = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    result = nlp(utterance)\n",
    "    score = result[0]['score']\n",
    "    if result[0]['label'] == 'NEGATIVE':\n",
    "        score = score * -1\n",
    "    \n",
    "    # talk(\"The score was {}\".format(score))\n",
    "    return score\n",
    "\n",
    "def breaking_the_ice(self):\n",
    "    leave_threshold = 5\n",
    "    for x in range(10):\n",
    "        user_input = input(\"Resp>>\")\n",
    "        responses = smalltalk(user_input)\n",
    "        sentiment = compute_sentiment(user_input)\n",
    "\n",
    "        print(responses, sentiment, leave_threshold)\n",
    "\n",
    "        leave_threshold =+ sentiment\n",
    "        if leave_threshold > 1:\n",
    "            print('hey, I just remembered to do something. See ya!')\n",
    "            return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /distilbert-base-uncased-finetuned-sst-2-english-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-special_tokens_map.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-tokenizer.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /distilbert-base-uncased-finetuned-sst-2-english-pytorch_model.bin HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE', 'score': 0.9998438954353333}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_sentiment([\"i'm doing well, thank you for asking. what are you up to this weekend?\"][0])['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /distilbert-base-uncased-finetuned-sst-2-english-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-special_tokens_map.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-tokenizer.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /distilbert-base-uncased-finetuned-sst-2-english-pytorch_model.bin HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    \n",
    "score = nlp('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /distilbert-base-uncased-finetuned-sst-2-english-pytorch_model.bin HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-special_tokens_map.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-tokenizer.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn.huggingface.co:443 \"HEAD /distilbert-base-uncased-finetuned-sst-2-english-pytorch_model.bin HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 'NEGATIVE', 'score': 0.9897596836090088}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_sentiment(utterance: str) -> dict:\n",
    "    nlp = pipeline(\"sentiment-analysis\")\n",
    "    score = nlp(utterance)[0]\n",
    "    # talk(\"The score was {}\".format(score))\n",
    "    return score\n",
    "\n",
    "\n",
    "compute_sentiment('well how about we do something next week?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/r2q2/Library/Jupyter/runtime/kernel-5a98a5c4-ea36-46df-8572-891a4c7eabb9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import plot_model\n",
    "from pyfiction.agents.ssaqn_agent import SSAQNAgent\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\"\"\"\n",
    "Load a model of an SSAQN agent trained on all six games\n",
    " and interactively test its Q-values of the state and action texts supplied by the user\n",
    "\"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    help='file path of a model to load',\n",
    "                    type=str,\n",
    "                    default='all.h5')\n",
    "# all.h5 contains a model trained on all six games (generalisation.py with argument of 0)\n",
    "\n",
    "args = parser.parse_args()\n",
    "model_path = args.model\n",
    "\n",
    "agent = SSAQNAgent(None)\n",
    "\n",
    "# Load or learn the vocabulary (random sampling on many games could be extremely slow)\n",
    "agent.initialize_tokens('vocabulary.txt')\n",
    "\n",
    "optimizer = RMSprop(lr=0.00001)\n",
    "\n",
    "embedding_dimensions = 16\n",
    "lstm_dimensions = 32\n",
    "dense_dimensions = 8\n",
    "\n",
    "agent.create_model(embedding_dimensions=embedding_dimensions,\n",
    "                   lstm_dimensions=lstm_dimensions,\n",
    "                   dense_dimensions=dense_dimensions,\n",
    "                   optimizer=optimizer)\n",
    "\n",
    "agent.model = load_model(model_path)\n",
    "\n",
    "print(\"Model\", model_path, \"loaded, now accepting state and actions texts and evaluating their Q-values.\")\n",
    "\n",
    "while True:\n",
    "    state = input(\"State: \")\n",
    "    action = input(\"Action: \")\n",
    "    print(\"Q-value: \", agent.q(state, action) * 30)\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GraphMachine' from 'transitions' (/usr/local/anaconda3/lib/python3.8/site-packages/transitions/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cc4aa289a10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!pip install transitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransitions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphMachine\u001b[0m \u001b[0;31m#, HierarchicalGraphMachine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GraphMachine' from 'transitions' (/usr/local/anaconda3/lib/python3.8/site-packages/transitions/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'But inside me, malfunctioning, I felt like I was in a dream. I was in a'}]\n"
     ]
    }
   ],
   "source": [
    "#while True:\n",
    "initial_prompt = 'But inside me, malfunction'\n",
    "print(poem_generator(initial_prompt, max_length=20, do_sample=False))\n",
    "#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I felt like I was in a dream. I was in a dream. I was in a dream'}]\n"
     ]
    }
   ],
   "source": [
    "print(poem_generator('I felt like I was in a dream.', max_length=20, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Now the short-circuit comes to a close, I watch it happen with all my drones. Some are like a ghost hovering over a hill, others are hovering above it. This could explain the unusual frequency that comes from the high frequency, low'}]\n"
     ]
    }
   ],
   "source": [
    "print(poem_generator('Now the short-circuit comes to a close, I watch it happen with all my drones.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'But inside me, malfunctioning, i’m like a fish out of water, a fish'}]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flexhusen_poem_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-704acbb904b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mflexhusen_poem_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i’m like a fish out of water, a fish'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flexhusen_poem_generator' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'im like a fish out of water a fish out of water\\n\\nI’m not sure'}]\n",
      "[{'generated_text': 'Im not sure if i am alone in this, but i am not alone in this.\\n\\nI am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married.'}]\n"
     ]
    }
   ],
   "source": [
    "print(felixhusen_poem_generator('im like a fish out of water a fish', max_length=20, do_sample=False))\n",
    "print(felixhusen_poem_generator('Im not sure', max_length=25, do_sample=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "model = AutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\", return_dict=True, mem_len=1024)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PROMPT_TO_REFINE = '''\n",
    "\n",
    "Saati Namba (ナンバー サーティ, Nanbā Sāti) Activated April 6, 1994\n",
    "Saati is the thirtieth A.I. program Hitoshi created, and is the first program that Hitoshi really liked.\n",
    "Through a freak lightning accident, she is transferred to the real world, and becomes Hitoshi's live-in girlfriend. In the real world she takes the family name \"Namba\" (難波), which is properly pronounced nanpa (\"whirlpool\"). In Japanese naming order her name, Namba Saati, is based on the English pronunciation of \"Number Thirty\" (Nanbā Sāti).\n",
    "A kind girl, size data 82-58-85 cm (32¾\"-23\"-34\") with thick eyebrows (A trait Akamatsu admitted to liking on women, citing Sae Isshiki as an example), and a bit naive, which does lead to frequent problems with Hitoshi. The most notable of which is that when she cooks meals she has the tendency to make them look exactly like the picture, regardless of the taste (having no taste buds), using everything from paint to colored markers until she receives a program upgrade that allows her to taste. She loves Hitoshi and is very loyal and supportive of him.\n",
    "While she is sweet and kind, she is similar to the Love Hina character Naru Narusegawa (whom she resembles) and Negima! Magister Negi Magi character Asuna Kagurazaka in that she beats up Hitoshi if he does something perverted.\n",
    "Hitoshi based her appearance on his mother.\n",
    "<eod> </s> <eos>\n",
    "'''\n",
    "prompt = \"What is your size?\"\n",
    "inputs = tokenizer.encode(PROMPT_TO_REFINE + prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=1500, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Im not sure', ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my', ' dream aflame', '. aflame, my name’s burn aflame. burn, burn, burn. i ask not', ' to be blamed', ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes', ' she felt like', ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak', ' due to fear', ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.', ' and once more']\n"
     ]
    }
   ],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded1b40bb52f472e8373fb29915c78f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c154b581c6740c596280531974df31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762dc09dc3ca46f887608c3daee04c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1340675298.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4fb7ff1c48b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0manswer_start_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_end_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     answer_start = torch.argmax(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0manswer_start_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )  # Get the most likely beginning of answer with the argmax of the score\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", return_dict=True)\n",
    "text = r\"\"\"\n",
    "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in 🤗 Transformers?\",\n",
    "    \"What does 🤗 Transformers provide?\",\n",
    "    \"🤗 Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if the body i have held is in good shape or bad, although i think it needs to be held.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Im not sure',\n",
       " ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my',\n",
       " ' dream aflame',\n",
       " '. aflame, my name’s burn aflame. burn, burn, burn. i ask not',\n",
       " ' to be blamed',\n",
       " ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes',\n",
       " ' she felt like',\n",
       " ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak',\n",
       " ' due to fear',\n",
       " ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.',\n",
       " ' and once more']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im not sure who it is but your imagination is right there where the crows flyLast night the wind caught my dream aflame. aflame my names burn aflame. burn burn burn. i ask not to be blamed and not wanted.For laura i love you so dearly we spent our nights together and sometimes she felt like stepping over a cliff my heart racing. i felt helpless then. not even being able to walk and speak due to fear of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again. and once more\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'helpless', score: 0.9145, start: 338, end: 346\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satti_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    user: str\n",
    "    unit_price: float\n",
    "    quantity_on_hand: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEvent(data):\n",
    "    event_log.append(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveEventLog(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7d794b4d0210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a conversation pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconversational_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conversational\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconversational_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "#Create a conversation pipeline\n",
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "for step in range(5):\n",
    "    conversational_pipeline(user_input)\n",
    "    \n",
    "    user_input = createEvent(input(\">> User:\"))\n",
    "    conversation = Conversation(user_input)\n",
    "    new_user_input_ids = DialoGPT_tokenizer.encode(input(\">> User:\") + DialoGPT_tokenizer.eos_token, return_tensors='pt')        \n",
    "    # append the new user input tokens to the chat history                                                     \n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids                                                                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/pytransitions/transitions#quickstart\n",
    "\n",
    "!pip install transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transitions in /usr/local/anaconda3/lib/python3.8/site-packages (0.8.5)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.8/site-packages (from transitions) (1.15.0)\n",
      "Resp>>How are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm good. just got back from a long day at work. how are you?\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1add0788914133a0e2db503bbce439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=629.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab7ade531b9487db57f41a83338d4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267844284.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a19468666e54b43979171f7836e552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f568c46ff8d4738bbcad6131420cb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'label': 'NEGATIVE', 'score': 0.9706663489341736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm good. just got back from a long day at work. how are you?\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transitions\n",
    "from transitions import Machine\n",
    "import random\n",
    "\n",
    "class Soul(object):\n",
    "\n",
    "    # Define some states. Most of the time, narcoleptic superheroes are just like\n",
    "    # everyone else. Except for...\n",
    "    #Note we should have first_impression be timedlocked by about an hour or less? \n",
    "    states = ['asleep', 'first_impression' ,'morning_question', \n",
    "              'hanging out', 'hungry','having fun','emberrased' , 'sweaty', 'saving the world', \n",
    "              'in love', 'indifferent', 'what_should_we_do']\n",
    "\n",
    "    def __init__(self, name):\n",
    "\n",
    "        # No anonymous superheroes on my watch! Every narcoleptic superhero gets\n",
    "        # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea.\n",
    "        self.name = name\n",
    "        \n",
    "        #Figure out outcome that would put you in the friendzone? \n",
    "        self.love_vector = 0 #-1 to 0 to +1\n",
    "        \n",
    "        # What have we accomplished today?\n",
    "        self.kittens_rescued = 0\n",
    "        \n",
    "        # Initialize the state machine\n",
    "        self.machine = Machine(model=self, states=Soul.states, initial='asleep')\n",
    "\n",
    "        # Add some transitions. We could also define these using a static list of\n",
    "        # dictionaries, as we did with states above, and then pass the list to\n",
    "        # the Machine initializer as the transitions= argument.\n",
    "\n",
    "        # At some point, every superhero must rise and shine.\n",
    "        self.machine.add_transition(trigger='wake_up', source='asleep', dest='hanging out', after='morning_question')\n",
    "        \n",
    "        self.machine.add_transition(trigger='morning_question', source='wake_up', dest='what_should_we_do'  )\n",
    "        \n",
    "        # Superheroes need to keep in shape.\n",
    "        self.machine.add_transition('work_out', 'hanging out', 'hungry')\n",
    "\n",
    "        # Those calories won't replenish themselves!\n",
    "        self.machine.add_transition('eat', 'hungry', 'hanging out')\n",
    "\n",
    "        # Superheroes are always on call. ALWAYS. But they're not always\n",
    "        # dressed in work-appropriate clothing.\n",
    "        self.machine.add_transition('distress_call', '*', 'saving the world',\n",
    "                         before='change_into_super_secret_costume')\n",
    "\n",
    "        # When they get off work, they're all sweaty and disgusting. But before\n",
    "        # they do anything else, they have to meticulously log their latest\n",
    "        # escapades. Because the legal department says so.\n",
    "        self.machine.add_transition('complete_mission', 'saving the world', 'sweaty',\n",
    "                         after='update_journal')\n",
    "\n",
    "        # Sweat is a disorder that can be remedied with water.\n",
    "        # Unless you've had a particularly long day, in which case... bed time!\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'asleep', conditions=['is_exhausted'])\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'hanging out')\n",
    "\n",
    "        # Our NarcolepticSuperhero can fall asleep at pretty much any time.\n",
    "        self.machine.add_transition('nap', '*', 'asleep')\n",
    "        \n",
    "        \n",
    "    def what_should_we_do(self):\n",
    "        print(\"What do you want to do today\")\n",
    "        \n",
    "    def talk(self):\n",
    "        print(\"Hello how are you?\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        \n",
    "        \n",
    "    def morning_question(self):\n",
    "        user_input = input(\"Resp>>\")\n",
    "        print(smalltalk(user_input))\n",
    "        print(compute_sentiment(user_input))\n",
    "        print(smalltalk(user_input))\n",
    "    \n",
    "    def update_journal(self):\n",
    "        \"\"\" Dear Diary, today I saved Mr. Whiskers. Again. \"\"\"\n",
    "        self.kittens_rescued += 1\n",
    "\n",
    "    @property\n",
    "    def is_exhausted(self):\n",
    "        \"\"\" Basically a coin toss. \"\"\"\n",
    "        return random.random() < 0.5\n",
    "\n",
    "    def change_into_super_secret_costume(self):\n",
    "        print(\"Beauty, eh?\")\n",
    "saati = Soul('saati')\n",
    "saati.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'test', 'test', 'test', 'test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfiction\n",
      "  Downloading pyfiction-0.1.2-py2.py3-none-any.whl (8.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.1 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (1.18.5)\n",
      "Collecting keras>=2.0.4\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: tensorflow>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (2.3.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "\u001b[K     |████████████████████████████████| 904 kB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (2.10.0)\n",
      "Collecting pydot\n",
      "  Using cached pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/anaconda3/lib/python3.8/site-packages (from keras>=2.0.4->pyfiction) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/anaconda3/lib/python3.8/site-packages (from keras>=2.0.4->pyfiction) (1.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (2.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (2.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (3.13.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.32.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.34.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/anaconda3/lib/python3.8/site-packages (from selenium->pyfiction) (1.25.9)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/anaconda3/lib/python3.8/site-packages (from pydot->pyfiction) (2.4.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (49.2.0.post20200714)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.22.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.1.0)\n",
      "Installing collected packages: keras, selenium, pydot, pyfiction\n",
      "Successfully installed keras-2.4.3 pydot-1.4.1 pyfiction-0.1.2 selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/r2q2/Library/Jupyter/runtime/kernel-7715a2d6-346c-4215-9f80-33643d13e544.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saati.update_journal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MachineError",
     "evalue": "\"Can't trigger event wake_up from state hanging out!\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMachineError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b8537f75d854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaati\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwake_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mtrigger\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# Machine._process should not be called somewhere else. That's why it should not be exposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# to Machine users.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, trigger)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transition_queue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# if trigger raises an Error, it has to be handled by the Machine.process caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempt to process events synchronously while transition queue is not empty!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_trigger\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mevent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMachineError\u001b[0m: \"Can't trigger event wake_up from state hanging out!\""
     ]
    }
   ],
   "source": [
    "saati.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/anaconda3/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: future in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.2)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (1.18.5)\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement bokeh==1.4.0, but you'll have bokeh 2.2.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement librosa==0.6.2, but you'll have librosa 0.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement unidecode==0.4.20, but you'll have unidecode 1.1.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: dataclasses\n",
      "Successfully installed dataclasses-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c pytorch pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
