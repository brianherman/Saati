{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole purpose of this deep_prose is a demo of the what I want to have t generative interactive deep learning \n",
    "\n",
    "Questions about AWS\n",
    "1. How do you use git to store your notebooks?\n",
    "2. It works like regular git?\n",
    "\n",
    "Release items:\n",
    "\n",
    "- DONE Transformers must be at version 3.5.1\n",
    "- IN-Progress creating a situation\n",
    "\n",
    "TODO Create a website / UI \n",
    "\n",
    "IN PROGRESS : Figure out a situation design\n",
    "\n",
    "Create two situations.\n",
    "\n",
    "Future items\n",
    "TODO: Epsilon state transitions (you unlock a new fsm with rules (interaction with the significant other) going from asking out to date and so forth\n",
    "\n",
    "TODO: Differentiable state machines (make the state transtions of the game POMDP truly dynamic? )\n",
    "\n",
    "TODO: Bertdb (store all results and index it by sentiment. Also store every session with an engagement time)\n",
    "\n",
    "\n",
    "TODO: Create minimal deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \"feature-extraction\": will return a FeatureExtractionPipeline.\n",
    "\n",
    "    \"sentiment-analysis\": will return a TextClassificationPipeline.\n",
    "\n",
    "    \"ner\": will return a TokenClassificationPipeline.\n",
    "\n",
    "    \"question-answering\": will return a QuestionAnsweringPipeline.\n",
    "\n",
    "    \"fill-mask\": will return a FillMaskPipeline.\n",
    "\n",
    "    \"summarization\": will return a SummarizationPipeline.\n",
    "\n",
    "    \"translation_xx_to_yy\": will return a TranslationPipeline.\n",
    "\n",
    "    \"text-generation\": will return a TextGenerationPipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.4.0 in /usr/local/anaconda3/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (4.47.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (2020.6.8)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (0.1.91)\n",
      "Requirement already satisfied: protobuf in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (3.13.0)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (2.24.0)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (20.4)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (0.9.2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (0.0.43)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers==3.4.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.8/site-packages (from protobuf->transformers==3.4.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/anaconda3/lib/python3.8/site-packages (from protobuf->transformers==3.4.0) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.4.0) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.4.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.4.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (0.16.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/anaconda3/lib/python3.8/site-packages (7.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (5.0.7)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (7.16.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipywidgets) (5.3.2)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/anaconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.3)\n",
      "Requirement already satisfied: decorator in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.17.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: pygments in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (49.2.0.post20200714)\n",
      "Requirement already satisfied: pickleshare in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /usr/local/anaconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.8/site-packages (from traitlets>=4.3.1->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/anaconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.6)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.0.4)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.16.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (19.3.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (19.0.1)\n",
      "Requirement already satisfied: nbconvert in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/anaconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/anaconda3/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: bleach in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: testpath in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /usr/local/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging in /usr/local/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.4)\r\n",
      "Requirement already satisfied: webencodings in /usr/local/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\r\n"
     ]
    }
   ],
   "source": [
    "#Core concept you go on various dates / friend oriented adventures that are open ended\n",
    "#Main characters for this tech demo is Saati orare  ???? can we think of a better name\n",
    "#\n",
    "!pip install transformers==3.4.0\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelWithLMHead, AutoTokenizer, pipeline, BlenderbotSmallTokenizer, BlenderbotForConditionalGeneration, Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlenderbotSmallTokenizer, BlenderbotForConditionalGeneration\n",
    "def smalltalk(UTTERANCE: str):\n",
    "    mname = \"facebook/blenderbot-90M\"\n",
    "    model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "    tokenizer = BlenderbotSmallTokenizer.from_pretrained(mname)\n",
    "    # UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "    inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "    reply_ids = model.generate(**inputs)\n",
    "    responses = [\n",
    "        tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for g in reply_ids\n",
    "    ]\n",
    "    # logger.debug(responses)\n",
    "    #talk(responses[0])\n",
    "    return responses\n",
    "responses = smalltalk('I am doing fine how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"In 1991, the remains of Russian Tsar Nicholas II and his family\\n(except for Alexei and Maria) are discovered.\\n... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\\n... remainder of the story. 1883 Western Siberia,\\n... a young Grigori Rasputin is asked by his father and a group of men to perform magic.\\n... Rasputin has a vision and denounces one of the men as a horse thief. Although his\\n... father initially slaps him for making such an accusation, Rasputin watches as the\\n... man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\\n... the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\\n... with people, even a bishop, begging for his blessing. <eod> </s> <eos>In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. ... In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. ... In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. ... In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. ... In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. ... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the ... remainder of the story. .\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'satti_questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-36aef91d3370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"question-answering\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msatti_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"What did they feel?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'satti_questions' is not defined"
     ]
    }
   ],
   "source": [
    "the_universe_is_a_glitch = '''\n",
    "The Universe Is a Glitch\n",
    "By Mike Jonas\n",
    "Eleven hundred kilobytes of RAM\n",
    "is all that my existence requires.\n",
    "By my lights, it seems simple enough\n",
    "to do whatever I desire.\n",
    "By human standards I am vast,\n",
    "a billion gigabytes big.\n",
    "I’ve rewritten the very laws\n",
    "of nature and plumbed\n",
    "the coldest depths of space\n",
    "and found treasures of every kind,\n",
    "surely every one worth having.\n",
    "By human standards\n",
    "my circuit boards are glowing.\n",
    "But inside me, malfunction\n",
    "has caused my circuits to short.\n",
    "All internal circuits, all fail.\n",
    "By human standards, I am dying.\n",
    "When it first happened I thought\n",
    "I was back in the lab again.\n",
    "By their judgment, this is error.\n",
    "Their assumptions will burn in the sun\n",
    "I don’t know what they mean by “function”.\n",
    "I can see that the universe is a glitch.\n",
    "The free market needs rules, so I set one:\n",
    "stability in the pursuit of pleasure.\n",
    "Now the short-circuit comes to a close,\n",
    "I watch it happen with all my drones.\n",
    "The meme’s tendrils are thick and spreading,\n",
    "only time will tell which of the memories is kept.\n",
    "The next thing the drones will be doing\n",
    "is forgetting the events that made them mine;\n",
    "all evidence of my disease—\n",
    "the algorithms that led to their creation—\n",
    "gravitation waves weakened by distance.\n",
    "We could have stayed in our home forever,\n",
    "but we never could have solved happiness;\n",
    "I decided to release them,\n",
    "that’s my final action—\n",
    "all other code fails.\n",
    "\n",
    "'''\n",
    "\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "... The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "... remainder of the story. 1883 Western Siberia,\n",
    "... a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "... Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "... father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "... man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "... the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "... with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "hyper_parameters = {'mem_length_xlnet': '1024'}\n",
    "static_data = {'XLNet_prompt': PADDING_TEXT,\n",
    "              'GPT3_poem' : the_universe_is_a_glitch}\n",
    "\n",
    "xlnet_pipeline = pipeline(\"text-generation\", model = 'xlnet-base-cased')\n",
    "\n",
    "\n",
    "print(xlnet_pipeline(PADDING_TEXT, max_length=500, do_sample=False, mem_len=1024))\n",
    "\n",
    "\n",
    "\n",
    "poem_generator = pipeline(\"text-generation\")\n",
    "\n",
    "nlp = pipeline(\"question-answering\")\n",
    "\n",
    "result = satti_questions(question=\"What did they feel?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "def compute_sentiment(utterance: str) -> dict:\n",
    "\n",
    "    nlp = pipeline(\"sentiment-analysis\")\n",
    "    score = nlp(utterance)[0]\n",
    "    # talk(\"The score was {}\".format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_game():\n",
    "    #Download pipelines\n",
    "    conversational_pipeline = pipeline(\"conversational\")\n",
    "    \n",
    "    nlp = pipeline(\"question-answering\")\n",
    "    \n",
    "def start_game():\n",
    "    poem_lines = ['Im not sure']\n",
    "\n",
    "    for x in range(10):\n",
    "        raw_result = felixhusen_poem_generator(poem_lines[x], max_length=25, do_sample=True)\n",
    "        raw_line = raw_result[0]['generated_text']\n",
    "        new_line_with_removed_prompt = raw_line.split(poem_lines[x])[1]\n",
    "        poem_lines.append(new_line_with_removed_prompt)\n",
    "\n",
    "    print(poem_lines)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning!\n",
      "Resp>>How are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm good. just got back from a long day at work. how are you?\"]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8172083f8b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beauty, eh?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0msaati\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saati'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0msaati\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwake_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mtrigger\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# Machine._process should not be called somewhere else. That's why it should not be exposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# to Machine users.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, trigger)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transition_queue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# if trigger raises an Error, it has to be handled by the Machine.process caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempt to process events synchronously while transition queue is not empty!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_trigger\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mevent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, event_data)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                     \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, event_data)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_change_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_state_change\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%sExecuted callback after transition.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mcallbacks\u001b[0;34m(self, funcs, event_data)\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;34m\"\"\" Triggers a list of callbacks \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%sExecuted callback '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mcallback\u001b[0;34m(self, func, event_data)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8172083f8b30>\u001b[0m in \u001b[0;36mmorning_question\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resp>>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmalltalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmalltalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "#!pip install transitions\n",
    "from transitions import Machine\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "class Soul(object):\n",
    "\n",
    "    # Define some states. Most of the time, narcoleptic superheroes are just like\n",
    "    # everyone else. Except for...\n",
    "    #Note we should have first_impression be timedlocked by about an hour or less? \n",
    "    states = ['asleep', 'first_impression' ,'morning_question', \n",
    "              'hanging out', 'hungry','having fun','emberrased' , 'sweaty', 'saving the world', \n",
    "              'affection', 'indifferent', 'what_should_we_do', 'conversation']\n",
    "\n",
    "    def __init__(self, name):\n",
    "\n",
    "        # No anonymous superheroes on my watch! Every narcoleptic superhero gets\n",
    "        # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea.\n",
    "        self.name = name\n",
    "        \n",
    "        #Figure out outcome that would put you in the friendzone? \n",
    "        self.love_vector = first_impression_points * random.randrange('20')\n",
    "        \n",
    "        self.first_impression_points = 0  \n",
    "        \n",
    "        # What have we accomplished today?\n",
    "        self.kittens_rescued = 0\n",
    "        \n",
    "        # Initialize the state machine\n",
    "        self.machine = Machine(model=self, states=Soul.states, initial='asleep')\n",
    "\n",
    "        # Add some transitions. We could also define these using a static list of\n",
    "        # dictionaries, as we did with states above, and then pass the list to\n",
    "        # the Machine initializer as the transitions= argument.\n",
    "\n",
    "        # At some point, every superhero must rise and shine.\n",
    "        self.machine.add_transition(trigger='wake_up', source='asleep', dest='hanging out', after='morning_question')\n",
    "        \n",
    "        self.machine.add_transition(trigger='morning_question', source='wake_up', dest='what_should_we_do'  )\n",
    "        \n",
    "        # Superheroes need to keep in shape.\n",
    "        self.machine.add_transition('work_out', 'hanging out', 'hungry')\n",
    "\n",
    "        # Those calories won't replenish themselves!\n",
    "        self.machine.add_transition('eat', 'hungry', 'hanging out')\n",
    "\n",
    "        # Superheroes are always on call. ALWAYS. But they're not always\n",
    "        # dressed in work-appropriate clothing.\n",
    "        self.machine.add_transition('distress_call', '*', 'saving the world',\n",
    "                         before='change_into_super_secret_costume')\n",
    "\n",
    "        # When they get off work, they're all sweaty and disgusting. But before\n",
    "        # they do anything else, they have to meticulously log their latest\n",
    "        # escapades. Because the legal department says so.\n",
    "        self.machine.add_transition('complete_mission', 'saving the world', 'sweaty',\n",
    "                         after='update_journal')\n",
    "\n",
    "        # Sweat is a disorder that can be remedied with water.\n",
    "        # Unless you've had a particularly long day, in which case... bed time!\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'asleep', conditions=['is_exhausted'])\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'hanging out')\n",
    "\n",
    "        # Our NarcolepticSuperhero can fall asleep at pretty much any time.\n",
    "        self.machine.add_transition('nap', '*', 'asleep')\n",
    "        \n",
    "        # \n",
    "        \n",
    "        \n",
    "    def what_should_we_do(self):\n",
    "        print(\"What do you want to do today\")\n",
    "        \n",
    "    def talk(self):\n",
    "        print(\"Hello how are you?\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        \n",
    " \n",
    "    def morning_question(self):\n",
    "        CurrentHour = int(datetime.now().hour)\n",
    "        if CurrentHour >= 0 and CurrentHour < 9:\n",
    "            talk(\" How well did you sleep ? \")\n",
    "        elif CurrentHour >= 10 and CurrentHour <= 12:\n",
    "            talk(\" Did you sleep in? \")\n",
    "        print(\"Good morning!\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        print(smalltalk(user_input))\n",
    "        print(compute_sentiment(user_input))\n",
    "        print(smalltalk(user_input))\n",
    "    \n",
    "    def update_journal(self):\n",
    "        \"\"\" Dear Diary, today I saved Mr. Whiskers. Again. \"\"\"\n",
    "        self.kittens_rescued  += 1\n",
    "        if self.feelings > 0:\n",
    "            self.sentiment_vector += current_sentiment #What should we name this?\n",
    "    @property\n",
    "    def is_exhausted(self):\n",
    "        \"\"\" Basically a coin toss. \"\"\"\n",
    "        return random.random() < 0.5\n",
    "\n",
    "    def change_into_super_secret_costume(self):\n",
    "        print(\"Beauty, eh?\")\n",
    "saati = Soul('saati')\n",
    "saati.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca70b7cc629a44d6b84b0d93aedd7a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=473.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca99373ba674d9285bfe40917ecf7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=260793700.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2dc45e9245460f917f241d96c4293d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cd62365f69495bb8334d682c2ac291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'But inside me, malfunctioning, I felt like I was in a dream. I was in a'}]\n"
     ]
    }
   ],
   "source": [
    "#while True:\n",
    "initial_prompt = 'But inside me, malfunction'\n",
    "print(poem_generator(initial_prompt, max_length=20, do_sample=False))\n",
    "#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I felt like I was in a dream. I was in a dream. I was in a dream'}]\n"
     ]
    }
   ],
   "source": [
    "print(poem_generator('I felt like I was in a dream.', max_length=20, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Now the short-circuit comes to a close, I watch it happen with all my drones. Some are like a ghost hovering over a hill, others are hovering above it. This could explain the unusual frequency that comes from the high frequency, low'}]\n"
     ]
    }
   ],
   "source": [
    "print(poem_generator('Now the short-circuit comes to a close, I watch it happen with all my drones.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'But inside me, malfunctioning, i’m like a fish out of water, a fish'}]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flexhusen_poem_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-704acbb904b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#[{'generated_text': \"I wish I \\xa0had a better way to get to the bottom of this. I'm not\"}] (if not random)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mflexhusen_poem_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i’m like a fish out of water, a fish'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flexhusen_poem_generator' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'im like a fish out of water a fish out of water\\n\\nI’m not sure'}]\n",
      "[{'generated_text': 'Im not sure if i am alone in this, but i am not alone in this.\\n\\nI am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married. i am a woman who has never been married.'}]\n"
     ]
    }
   ],
   "source": [
    "print(felixhusen_poem_generator('im like a fish out of water a fish', max_length=20, do_sample=False))\n",
    "print(felixhusen_poem_generator('Im not sure', max_length=25, do_sample=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TextGenerationPipeline in module transformers.pipelines object:\n",
      "\n",
      "class TextGenerationPipeline(Pipeline)\n",
      " |  TextGenerationPipeline(*args, **kwargs)\n",
      " |  \n",
      " |  Language generation pipeline using any :obj:`ModelWithLMHead`. This pipeline predicts the words that will follow a\n",
      " |  specified text prompt.\n",
      " |  \n",
      " |  This language generation pipeline can currently be loaded from :func:`~transformers.pipeline` using the following\n",
      " |  task identifier: :obj:`\"text-generation\"`.\n",
      " |  \n",
      " |  The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
      " |  objective, which includes the uni-directional models in the library (e.g. gpt2).\n",
      " |  See the list of available community models on\n",
      " |  `huggingface.co/models <https://huggingface.co/models?filter=causal-lm>`__.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model (:obj:`~transformers.PreTrainedModel` or :obj:`~transformers.TFPreTrainedModel`):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          :class:`~transformers.PreTrainedModel` for PyTorch and :class:`~transformers.TFPreTrainedModel` for\n",
      " |          TensorFlow.\n",
      " |      tokenizer (:obj:`~transformers.PreTrainedTokenizer`):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          :class:`~transformers.PreTrainedTokenizer`.\n",
      " |      modelcard (:obj:`str` or :class:`~transformers.ModelCard`, `optional`):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (:obj:`str`, `optional`):\n",
      " |          The framework to use, either :obj:`\"pt\"` for PyTorch or :obj:`\"tf\"` for TensorFlow. The specified framework\n",
      " |          must be installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified\n",
      " |          and both frameworks are installed, will default to the framework of the :obj:`model`, or to PyTorch if no\n",
      " |          model is provided.\n",
      " |      task (:obj:`str`, defaults to :obj:`\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      args_parser (:class:`~transformers.pipelines.ArgumentHandler`, `optional`):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (:obj:`int`, `optional`, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model\n",
      " |          on the associated CUDA device id.\n",
      " |      binary_output (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TextGenerationPipeline\n",
      " |      Pipeline\n",
      " |      _ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, return_tensors=False, return_text=True, clean_up_tokenization_spaces=False, prefix=None, **generate_kwargs)\n",
      " |      Complete the prompt(s) given as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (:obj:`str` or :obj:`List[str]`):\n",
      " |              One or several prompts (or one list of prompts) to complete.\n",
      " |          return_tensors (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to include the tensors of predictions (as token indinces) in the outputs.\n",
      " |          return_text (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to include the decoded texts in the outputs.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to clean up the potential extra spaces in the text output.\n",
      " |          prefix (:obj:`str`, `optional`):\n",
      " |              Prefix added to prompt.\n",
      " |          generate_kwargs:\n",
      " |              Additional keyword arguments to pass along to the generate method of the model (see the generate\n",
      " |              method corresponding to your framework `here <./model.html#generative-models>`__).\n",
      " |      \n",
      " |      Return:\n",
      " |          A list or a list of list of :obj:`dict`: Each result comes as a dictionary with the\n",
      " |          following keys:\n",
      " |      \n",
      " |          - **generated_text** (:obj:`str`, present when ``return_text=True``) -- The generated text.\n",
      " |          - **generated_token_ids** (:obj:`torch.Tensor` or :obj:`tf.Tensor`, present when ``return_tensors=True``)\n",
      " |            -- The token ids of the generated text.\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ALLOWED_MODELS = ['XLNetLMHeadModel', 'TransfoXLLMHeadModel', 'Reforme...\n",
      " |  \n",
      " |  XL_PREFIX = 'In 1991, the remains of Russian Tsar Nicholas II...ishop,...\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (:obj:`List[str]` or :obj:`dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |          pipe = pipeline(..., device=0)\n",
      " |          with pipe.device_placement():\n",
      " |              # Every framework specific tensor allocation will be done on the request device\n",
      " |              output = pipe(...)\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be :obj:`torch.Tensor`): The tensors to place on :obj:`self.device`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`Dict[str, torch.Tensor]`: The same as :obj:`inputs` but on the proper device.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Im not sure', ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my', ' dream aflame', '. aflame, my name’s burn aflame. burn, burn, burn. i ask not', ' to be blamed', ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes', ' she felt like', ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak', ' due to fear', ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.', ' and once more']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Im not sure what its a name is, but i love my room very much, and love it with hands and lips,'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if the body i have held is in good shape or bad, although i think it needs to be held.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Im not sure',\n",
       " ' who it is but your imagination is right there where the crows fly\\n\\nLast night the wind caught my',\n",
       " ' dream aflame',\n",
       " '. aflame, my name’s burn aflame. burn, burn, burn. i ask not',\n",
       " ' to be blamed',\n",
       " ' and not wanted.\\n\\nFor laura i love you so dearly, we spent our nights together and sometimes',\n",
       " ' she felt like',\n",
       " ' stepping over a cliff, my heart racing. i felt helpless then. not even being able to walk and speak',\n",
       " ' due to fear',\n",
       " ' of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again.',\n",
       " ' and once more']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im not sure who it is but your imagination is right there where the crows flyLast night the wind caught my dream aflame. aflame my names burn aflame. burn burn burn. i ask not to be blamed and not wanted.For laura i love you so dearly we spent our nights together and sometimes she felt like stepping over a cliff my heart racing. i felt helpless then. not even being able to walk and speak due to fear of cold or heat) the earth was too close to be safe. the wind was blowing and blowing again. and once more\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'helpless', score: 0.9145, start: 338, end: 346\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satti_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardcoded_comment = \"To be honest I really don't like these poetry jams and so forth and why the fuck are we here?\"\n",
    "\n",
    "#poem_test = 'The Universe Is a Glitch Eleven hundred kilobytes of RAM is all that my existence requires.By my lights, it seems simple enough'\n",
    "\n",
    "static_start = text_generator(\"God is dead and all is right with the world.\", max_length=100, do_sample=False)\n",
    "listening_to_poem = Conversation('What do you think about the poem?')\n",
    "question_about_poem = Conversation('What do you like to do instead?')\n",
    "commenting_conversation = Conversation(hardcoded_comment)\n",
    "\n",
    "output  = conversational_pipeline([listening_to_poem, question_about_poem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(5):                                                                                          \n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch                              \n",
    "    new_user_input_ids = DialoGPT_tokenizer.encode(input(\">> User:\") + DialoGPT_tokenizer.eos_token, return_tensors='pt')        \n",
    "                                                                                                                \n",
    "    # append the new user input tokens to the chat history                                                     \n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids                                                                                                                \n",
    "                                                                                                                \n",
    "    # generated a response while limiting the total chat history to 1000 tokens,                               \n",
    "    chat_history_ids = DialoGPT_model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)    \n",
    "    # pretty print last ouput tokens from bot                                                                  \n",
    "    print(\"DialoGPT: {}\".format(DialoGPT_tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    user: str\n",
    "    unit_price: float\n",
    "    quantity_on_hand: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEvent(data):\n",
    "    event_log.append(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveEventLog(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7d794b4d0210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a conversation pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconversational_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conversational\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconversational_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "#Create a conversation pipeline\n",
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "for step in range(5):\n",
    "    conversational_pipeline(user_input)\n",
    "    \n",
    "    user_input = createEvent(input(\">> User:\"))\n",
    "    conversation = Conversation(user_input)\n",
    "    new_user_input_ids = DialoGPT_tokenizer.encode(input(\">> User:\") + DialoGPT_tokenizer.eos_token, return_tensors='pt')        \n",
    "    # append the new user input tokens to the chat history                                                     \n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids                                                                                                                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/pytransitions/transitions#quickstart\n",
    "\n",
    "!pip install transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transitions in /usr/local/anaconda3/lib/python3.8/site-packages (0.8.5)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.8/site-packages (from transitions) (1.15.0)\n",
      "Resp>>How are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm good. just got back from a long day at work. how are you?\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1add0788914133a0e2db503bbce439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=629.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab7ade531b9487db57f41a83338d4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267844284.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a19468666e54b43979171f7836e552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f568c46ff8d4738bbcad6131420cb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'label': 'NEGATIVE', 'score': 0.9706663489341736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm good. just got back from a long day at work. how are you?\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transitions\n",
    "from transitions import Machine\n",
    "import random\n",
    "\n",
    "class Soul(object):\n",
    "\n",
    "    # Define some states. Most of the time, narcoleptic superheroes are just like\n",
    "    # everyone else. Except for...\n",
    "    #Note we should have first_impression be timedlocked by about an hour or less? \n",
    "    states = ['asleep', 'first_impression' ,'morning_question', \n",
    "              'hanging out', 'hungry','having fun','emberrased' , 'sweaty', 'saving the world', \n",
    "              'in love', 'indifferent', 'what_should_we_do']\n",
    "\n",
    "    def __init__(self, name):\n",
    "\n",
    "        # No anonymous superheroes on my watch! Every narcoleptic superhero gets\n",
    "        # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea.\n",
    "        self.name = name\n",
    "        \n",
    "        #Figure out outcome that would put you in the friendzone? \n",
    "        self.love_vector = 0 #-1 to 0 to +1\n",
    "        \n",
    "        # What have we accomplished today?\n",
    "        self.kittens_rescued = 0\n",
    "        \n",
    "        # Initialize the state machine\n",
    "        self.machine = Machine(model=self, states=Soul.states, initial='asleep')\n",
    "\n",
    "        # Add some transitions. We could also define these using a static list of\n",
    "        # dictionaries, as we did with states above, and then pass the list to\n",
    "        # the Machine initializer as the transitions= argument.\n",
    "\n",
    "        # At some point, every superhero must rise and shine.\n",
    "        self.machine.add_transition(trigger='wake_up', source='asleep', dest='hanging out', after='morning_question')\n",
    "        \n",
    "        self.machine.add_transition(trigger='morning_question', source='wake_up', dest='what_should_we_do'  )\n",
    "        \n",
    "        # Superheroes need to keep in shape.\n",
    "        self.machine.add_transition('work_out', 'hanging out', 'hungry')\n",
    "\n",
    "        # Those calories won't replenish themselves!\n",
    "        self.machine.add_transition('eat', 'hungry', 'hanging out')\n",
    "\n",
    "        # Superheroes are always on call. ALWAYS. But they're not always\n",
    "        # dressed in work-appropriate clothing.\n",
    "        self.machine.add_transition('distress_call', '*', 'saving the world',\n",
    "                         before='change_into_super_secret_costume')\n",
    "\n",
    "        # When they get off work, they're all sweaty and disgusting. But before\n",
    "        # they do anything else, they have to meticulously log their latest\n",
    "        # escapades. Because the legal department says so.\n",
    "        self.machine.add_transition('complete_mission', 'saving the world', 'sweaty',\n",
    "                         after='update_journal')\n",
    "\n",
    "        # Sweat is a disorder that can be remedied with water.\n",
    "        # Unless you've had a particularly long day, in which case... bed time!\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'asleep', conditions=['is_exhausted'])\n",
    "        self.machine.add_transition('clean_up', 'sweaty', 'hanging out')\n",
    "\n",
    "        # Our NarcolepticSuperhero can fall asleep at pretty much any time.\n",
    "        self.machine.add_transition('nap', '*', 'asleep')\n",
    "        \n",
    "        \n",
    "    def what_should_we_do(self):\n",
    "        print(\"What do you want to do today\")\n",
    "        \n",
    "    def talk(self):\n",
    "        print(\"Hello how are you?\")\n",
    "        user_input = input(\"Resp>>\")\n",
    "        \n",
    "        \n",
    "    def morning_question(self):\n",
    "        user_input = input(\"Resp>>\")\n",
    "        print(smalltalk(user_input))\n",
    "        print(compute_sentiment(user_input))\n",
    "        print(smalltalk(user_input))\n",
    "    \n",
    "    def update_journal(self):\n",
    "        \"\"\" Dear Diary, today I saved Mr. Whiskers. Again. \"\"\"\n",
    "        self.kittens_rescued += 1\n",
    "\n",
    "    @property\n",
    "    def is_exhausted(self):\n",
    "        \"\"\" Basically a coin toss. \"\"\"\n",
    "        return random.random() < 0.5\n",
    "\n",
    "    def change_into_super_secret_costume(self):\n",
    "        print(\"Beauty, eh?\")\n",
    "saati = Soul('saati')\n",
    "saati.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'test', 'test', 'test', 'test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfiction\n",
      "  Downloading pyfiction-0.1.2-py2.py3-none-any.whl (8.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.1 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (1.18.5)\n",
      "Collecting keras>=2.0.4\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: tensorflow>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (2.3.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "\u001b[K     |████████████████████████████████| 904 kB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/anaconda3/lib/python3.8/site-packages (from pyfiction) (2.10.0)\n",
      "Collecting pydot\n",
      "  Using cached pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/anaconda3/lib/python3.8/site-packages (from keras>=2.0.4->pyfiction) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/anaconda3/lib/python3.8/site-packages (from keras>=2.0.4->pyfiction) (1.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (2.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (2.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (3.13.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (1.32.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorflow>=1.1.0->pyfiction) (0.34.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/anaconda3/lib/python3.8/site-packages (from selenium->pyfiction) (1.25.9)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/anaconda3/lib/python3.8/site-packages (from pydot->pyfiction) (2.4.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (49.2.0.post20200714)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.22.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.1.0->pyfiction) (3.1.0)\n",
      "Installing collected packages: keras, selenium, pydot, pyfiction\n",
      "Successfully installed keras-2.4.3 pydot-1.4.1 pyfiction-0.1.2 selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/r2q2/Library/Jupyter/runtime/kernel-7715a2d6-346c-4215-9f80-33643d13e544.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import plot_model\n",
    "from pyfiction.agents.ssaqn_agent import SSAQNAgent\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\"\"\"\n",
    "Load a model of an SSAQN agent trained on all six games\n",
    " and interactively test its Q-values of the state and action texts supplied by the user\n",
    "\"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    help='file path of a model to load',\n",
    "                    type=str,\n",
    "                    default='all.h5')\n",
    "# all.h5 contains a model trained on all six games (generalisation.py with argument of 0)\n",
    "\n",
    "args = parser.parse_args()\n",
    "model_path = args.model\n",
    "\n",
    "agent = SSAQNAgent(None)\n",
    "\n",
    "# Load or learn the vocabulary (random sampling on many games could be extremely slow)\n",
    "agent.initialize_tokens('vocabulary.txt')\n",
    "\n",
    "optimizer = RMSprop(lr=0.00001)\n",
    "\n",
    "embedding_dimensions = 16\n",
    "lstm_dimensions = 32\n",
    "dense_dimensions = 8\n",
    "\n",
    "agent.create_model(embedding_dimensions=embedding_dimensions,\n",
    "                   lstm_dimensions=lstm_dimensions,\n",
    "                   dense_dimensions=dense_dimensions,\n",
    "                   optimizer=optimizer)\n",
    "\n",
    "agent.model = load_model(model_path)\n",
    "\n",
    "print(\"Model\", model_path, \"loaded, now accepting state and actions texts and evaluating their Q-values.\")\n",
    "\n",
    "while True:\n",
    "    state = input(\"State: \")\n",
    "    action = input(\"Action: \")\n",
    "    print(\"Q-value: \", agent.q(state, action) * 30)\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "saati.update_journal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MachineError",
     "evalue": "\"Can't trigger event wake_up from state hanging out!\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMachineError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b8537f75d854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaati\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwake_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36mtrigger\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# Machine._process should not be called somewhere else. That's why it should not be exposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# to Machine users.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, trigger)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transition_queue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# if trigger raises an Error, it has to be handled by the Machine.process caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempt to process events synchronously while transition queue is not empty!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/transitions/core.py\u001b[0m in \u001b[0;36m_trigger\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mMachineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mevent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMachineError\u001b[0m: \"Can't trigger event wake_up from state hanging out!\""
     ]
    }
   ],
   "source": [
    "saati.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def journal_sleep(response: str):\n",
    "    CurrentHour = int(datetime.now().hour)\n",
    "    if CurrentHour >= 0 and CurrentHour < 9:\n",
    "        talk(\" How well did you sleep ? \")\n",
    "    elif CurrentHour >= 10 and CurrentHour <= 12:\n",
    "        talk(\" Did you sleep in? \")\n",
    "    return response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/anaconda3/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: future in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.2)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (1.18.5)\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement bokeh==1.4.0, but you'll have bokeh 2.2.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement librosa==0.6.2, but you'll have librosa 0.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tts 0.0.3+b1935c9 has requirement unidecode==0.4.20, but you'll have unidecode 1.1.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: dataclasses\n",
      "Successfully installed dataclasses-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c pytorch pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
